"""
Plain RL Training Script with LSTM Architecture
Uses REINFORCE (policy gradient) algorithm with LSTM neural network
Same state, action, and reward as portfolio_env

This script implements a simpler RL algorithm than PPO while keeping the LSTM architecture
for temporal modeling of the portfolio environment.

BALANCED LEARNING IMPROVEMENTS:
- Moderate action std (0.18 → 0.12) with gradual warmup for stable exploration
- Learning rate scheduling (cosine decay) for gradual learning
- Higher gamma (0.975) for smoother credit assignment in volatile markets
- Balanced entropy (0.007 → 0.0015) with slow decay to prevent brittle strategies
- Reward normalization and clipping [-8, 8] to prevent outlier hijacking
- Smaller network (128 hidden, 1 LSTM layer) for better generalization
- Robust early stopping metrics (Sharpe ratio, max drawdown) instead of raw returns
- Gradient clipping (norm 6.0) for stable updates
"""

import os
import sys
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any, Set, NamedTuple, Tuple
import time

# Configure JAX to use CPU only (Metal GPU not compatible with current JAX version)
os.environ['JAX_PLATFORM_NAME'] = 'cpu'

# Setup logging early
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Fix for JAX 0.6.2 + Flax 0.8.4 compatibility issue
def fix_evaltrace_error():
    """Fix EvalTrace level attribute error in JAX 0.6.2 + Flax 0.8.4"""
    try:
        import flax.core.tracers as tracers
        
        def patched_trace_level(main):
            """Patched version of trace_level that handles missing level attribute"""
            if main:
                if hasattr(main, 'level'):
                    return main.level
                else:
                    return 0
            return float('-inf')
        
        tracers.trace_level = patched_trace_level
        logger.info("Applied monkey patch to fix EvalTrace level attribute error")
    except Exception as e:
        logger.warning(f"Could not apply EvalTrace fix: {e}")

# Apply the fix immediately
fix_evaltrace_error()

import jax
import jax.numpy as jnp
import jax.random as random
import numpy as np
import pandas as pd
from functools import partial
import chex

# Import LSTM architecture and environment
from train_ppo_lstm import ActorCriticLSTM, LSTMState
from finagent.environment.portfolio_env import (
    JAXPortfolioDataLoader,
    JAXVectorizedPortfolioEnv,
    EnvState
)
import optax
from flax.training import train_state
import distrax

# Optional imports for wandb
try:
    import wandb
except ImportError:
    wandb = None


class FeatureSelector:
    """Feature selector for different feature categories"""
    
    def __init__(self):
        # Define feature categories based on actual CSV data and portfolio_env.py analysis
        self.feature_categories = {
            'ohlcv': {
                'description': 'Basic OHLCV price data and simple derived features',
                'features': [
                    # Basic OHLCV (available in CSV)
                    'open', 'high', 'low', 'close', 'volume', 'vwap',
                    
                    # Simple returns (generated by our custom engineer_features)
                    'returns_1d', 'returns_3d', 'returns_5d', 'returns_10d', 'returns_20d',
                    'log_returns_1d', 'log_returns_5d',
                    
                    # Basic price action features (generated by our custom engineer_features)
                    'overnight_gap', 'daily_range', 'close_position',
                    
                    # Lag features (pre-computed in CSV)
                    'open_lag_1', 'open_lag_2', 'open_lag_3', 'open_lag_5', 'open_lag_10', 'open_lag_20',
                    'high_lag_1', 'high_lag_2', 'high_lag_3', 'high_lag_5', 'high_lag_10', 'high_lag_20',
                    'low_lag_1', 'low_lag_2', 'low_lag_3', 'low_lag_5', 'low_lag_10', 'low_lag_20',
                    'close_lag_1', 'close_lag_2', 'close_lag_3', 'close_lag_5', 'close_lag_10', 'close_lag_20',
                    'volume_lag_1', 'volume_lag_2', 'volume_lag_3', 'volume_lag_5', 'volume_lag_10', 'volume_lag_20',
                    
                    # Rolling statistics (pre-computed in CSV)
                    'open_rolling_mean_5', 'open_rolling_mean_20', 'open_rolling_std_20',
                    'high_rolling_mean_5', 'high_rolling_mean_20', 'high_rolling_std_20',
                    'low_rolling_mean_5', 'low_rolling_mean_20', 'low_rolling_std_20',
                    'close_rolling_mean_5', 'close_rolling_mean_20', 'close_rolling_std_20',
                    'close_momentum_5', 'close_momentum_20',
                    'volume_rolling_mean_5', 'volume_rolling_mean_20', 'volume_rolling_std_20'
                ]
            },
            
            'technical': {
                'description': 'Technical indicators and algorithmic trading signals',
                'features': [
                    # Moving averages (pre-computed in CSV)
                    'dma_50', 'dma_200',
                    'dma_50_lag_1', 'dma_50_lag_2', 'dma_50_lag_3', 'dma_50_lag_5', 'dma_50_lag_10', 'dma_50_lag_20',
                    'dma_200_lag_1', 'dma_200_lag_2', 'dma_200_lag_3', 'dma_200_lag_5', 'dma_200_lag_10', 'dma_200_lag_20',
                    'dma_50_rolling_mean_5', 'dma_50_rolling_mean_20', 'dma_50_rolling_std_20',
                    'dma_200_rolling_mean_5', 'dma_200_rolling_mean_20', 'dma_200_rolling_std_20',
                    'dma_cross', 'dma_distance', 'volume_price_trend',
                    
                    # RSI (pre-computed in CSV)
                    'rsi_14',
                    
                    # Volatility features (will be generated by engineer_features)
                    'volatility_5d', 'volatility_10d', 'volatility_20d', 'volatility_30d', 'volatility_60d',
                    'vol_ratio_short_long', 'vol_ratio_5_20',
                    
                    # Momentum features (will be generated)
                    'momentum_5d', 'momentum_10d', 'momentum_20d', 'momentum_60d',
                    'momentum_acceleration_10d',
                    
                    # Moving average features (will be generated)
                    'ma_convergence', 'ma_trend_strength', 'price_position_20d', 
                    'price_above_ma50', 'price_above_ma200',
                    
                    # RSI signals (will be generated)
                    'rsi_oversold', 'rsi_overbought', 'rsi_bullish_divergence', 'rsi_bearish_divergence',
                    'rsi_zscore_20d',
                    
                    # Bollinger Bands (will be generated)
                    'bb_position', 'bb_squeeze', 'bb_breakout_up', 'bb_breakout_down',
                    
                    # Mean reversion signals (will be generated)
                    'price_zscore_20d', 'price_zscore_60d', 'volume_zscore_20d', 'volume_zscore_60d',
                    'price_deviation_50d', 'price_deviation_200d',
                    'mean_reversion_signal_50d', 'mean_reversion_signal_200d',
                    
                    # Breakout signals (will be generated)
                    'price_breakout_20d', 'price_breakdown_20d', 'volume_breakout_20d', 'volume_spike',
                    
                    # Trend following (will be generated)
                    'ma_cross_bullish', 'ma_cross_bearish',
                    
                    # Volatility regime (will be generated)
                    'high_vol_regime', 'low_vol_regime', 'vol_expansion', 'vol_contraction',
                    
                    # Candlestick patterns (will be generated)
                    'body_ratio', 'upper_wick_ratio', 'lower_wick_ratio',
                    'doji_pattern', 'hammer_pattern', 'shooting_star_pattern',
                    
                    # Volume analysis (will be generated)
                    'volume_price_momentum', 'volume_ratio_5d', 'volume_ratio_20d',
                    'volume_trend_10d', 'volume_confirms_price', 'volume_divergence',
                    
                    # Signal aggregation (will be generated)
                    'bullish_signals', 'bearish_signals', 'net_signal_strength',
                    
                    # Risk-adjusted metrics (will be generated)
                    'risk_adjusted_momentum', 'volume_confirmed_trend',
                    
                    # Regime detection (will be generated)
                    'vol_regime_change', 'trend_regime',
                    
                    # Cross-sectional features (will be generated)
                    'momentum_rank_proxy', 'vol_rank_proxy'
                ]
            },
            
            'financial': {
                'description': 'Financial statement metrics and fundamental analysis',
                'features': [
                    # Core Income Statement Features (from CSV)
                    'metric_Revenue', 'metric_TotalRevenue', 'metric_CostofRevenueTotal', 'metric_GrossProfit',
                    'metric_OperatingIncome', 'metric_NetIncomeBeforeTaxes', 'metric_NetIncomeAfterTaxes',
                    'metric_NetIncome', 'metric_DilutedNetIncome', 'metric_DilutedWeightedAverageShares',
                    'metric_DilutedEPSExcludingExtraOrdItems', 'metric_DPS-CommonStockPrimaryIssue',
                    
                    # Core Balance Sheet Features (from CSV)
                    'metric_Cash', 'metric_ShortTermInvestments', 'metric_CashandShortTermInvestments',
                    'metric_TotalCurrentAssets', 'metric_TotalAssets', 'metric_TotalCurrentLiabilities',
                    'metric_TotalLiabilities', 'metric_TotalEquity', 'metric_TotalCommonSharesOutstanding',
                    
                    # Core Cash Flow Features (from CSV)
                    'metric_CashfromOperatingActivities', 'metric_CapitalExpenditures', 
                    'metric_CashfromInvestingActivities', 'metric_CashfromFinancingActivities',
                    'metric_NetChangeinCash', 'metric_TotalCashDividendsPaid',
                    
                    # Key Financial Metrics (from CSV)
                    'metric_freeCashFlowtrailing12Month', 'metric_freeCashFlowMostRecentFiscalYear',
                    'metric_periodLength', 'metric_periodType',
                    
                    # Additional financial metrics (from CSV)
                    'metric_pPerEExcludingExtraordinaryItemsMostRecentFiscalYear',
                    'metric_currentDividendYieldCommonStockPrimaryIssueLTM',
                    'metric_priceToBookMostRecentFiscalYear',
                    'metric_priceToFreeCashFlowPerShareTrailing12Months',
                    'metric_pPerEBasicExcludingExtraordinaryItemsTTM',
                    'metric_pPerEIncludingExtraordinaryItemsTTM',
                    'metric_returnOnAverageEquityMostRecentFiscalYear',
                    'metric_returnOnInvestmentMostRecentFiscalYear',
                    'metric_netProfitMarginPercentTrailing12Month',
                    'metric_operatingMarginTrailing12Month',
                    'metric_grossMarginTrailing12Month',
                    'metric_currentRatioMostRecentFiscalYear',
                    'metric_quickRatioMostRecentFiscalYear',
                    'metric_totalDebtPerTotalEquityMostRecentFiscalYear',
                    'metric_netInterestCoverageMostRecentFiscalYear',
                    'metric_marketCap',
                    'metric_beta'
                ]
            },
            
            'sentiment': {
                'description': 'News and social media sentiment indicators',
                'features': [
                    # Reddit sentiment features (from CSV)
                    'reddit_title_sentiments_mean', 'reddit_title_sentiments_std',
                    'reddit_body_sentiments', 'reddit_body_sentiments_std',
                    'reddit_score_mean', 'reddit_score_sum', 'reddit_posts_count', 'reddit_comments_sum',
                    
                    # News sentiment features (from CSV)
                    'news_sentiment_mean', 'news_articles_count', 'news_sentiment_std', 'news_sources',
                    
                    # Sentiment features that will be generated by engineer_features
                    'sentiment_momentum_3d', 'sentiment_momentum_5d',
                    'sentiment_extreme_positive', 'sentiment_extreme_negative'
                ]
            }
        }
    
    def get_features_for_combination(self, combination: str) -> List[str]:
        """
        Get list of features for a given combination string.
        
        Args:
            combination: Feature combination string like 'ohlcv+technical' or 'all'
            
        Returns:
            List of feature names
        """
        if combination.lower() == 'all':
            # Return all features from all categories
            all_features = []
            for category_features in self.feature_categories.values():
                all_features.extend(category_features['features'])
            return list(set(all_features))  # Remove duplicates
        
        # Parse combination string
        categories = [cat.strip().lower() for cat in combination.split('+')]
        
        # Validate categories
        valid_categories = set(self.feature_categories.keys())
        invalid_categories = set(categories) - valid_categories
        if invalid_categories:
            raise ValueError(f"Invalid feature categories: {invalid_categories}. "
                           f"Valid categories are: {list(valid_categories)}")
        
        # Combine features from selected categories
        selected_features = []
        for category in categories:
            selected_features.extend(self.feature_categories[category]['features'])
        
        # Remove duplicates and ensure 'close' is always first
        selected_features = list(set(selected_features))
        if 'close' in selected_features:
            selected_features.remove('close')
            selected_features = ['close'] + selected_features
        
        return selected_features
    
    def print_available_combinations(self):
        """Print available feature combinations"""
        print("\n=== Available Feature Categories ===")
        for category, info in self.feature_categories.items():
            print(f"\n{category.upper()}: {info['description']}")
            print(f"  Features ({len(info['features'])}): {', '.join(info['features'][:5])}...")
        
        print(f"\n=== Example Combinations ===")
        print("• ohlcv - Basic price data only")
        print("• technical - Technical indicators only")
        print("• financial - Financial metrics only")
        print("• sentiment - Sentiment data only")
        print("• ohlcv+technical - Price data + technical indicators")
        print("• ohlcv+financial - Price data + financial metrics")
        print("• ohlcv+sentiment - Price data + sentiment")
        print("• technical+financial - Technical + financial indicators")
        print("• technical+sentiment - Technical + sentiment indicators")
        print("• financial+sentiment - Financial + sentiment indicators")
        print("• ohlcv+technical+financial - Price + technical + financial")
        print("• ohlcv+technical+sentiment - Price + technical + sentiment")
        print("• ohlcv+financial+sentiment - Price + financial + sentiment")
        print("• technical+financial+sentiment - All except basic price data")
        print("• all - All available features")


# -----------------------
# Utilities
# -----------------------
def has_columns(df: pd.DataFrame, cols: List[str]) -> bool:
    """
    Safely check whether DataFrame has all columns in `cols`.
    Avoids ambiguous truth-value operations with pandas Index.
    """
    return set(cols).issubset(set(df.columns))


# -----------------------
# CustomDataLoader
# -----------------------
class CustomDataLoader(JAXPortfolioDataLoader):
    """
    Custom data loader that only generates requested features and prevents
    the parent loader from appending a large default feature set.
    """
    def __init__(self, selected_features: List[str], *args, **kwargs):
        # Store selected features explicitly
        self.selected_features = [str(f) for f in selected_features]
        # Force disable 'use_all_features' behaviour by default
        kwargs.setdefault('use_all_features', False)
        # Set the features attribute (so parent may see it)
        kwargs.setdefault('features', self.selected_features)
        try:
            super().__init__(*args, **kwargs)
        except TypeError:
            # Defensive: if parent has a different signature, try without args
            try:
                super().__init__()
                # set attributes directly
                self.features = self.selected_features
                self.use_all_features = False
            except Exception as e:
                logger.error(f"Could not call parent JAXPortfolioDataLoader.__init__: {e}")
                raise

        # Ensure we don't later get overwritten accidentally
        self.features = list(self.selected_features)
        self.use_all_features = False

    def engineer_features(self, df: pd.DataFrame, stock: str) -> pd.DataFrame:
        """
        Engineer only requested features. Returns DataFrame with exactly
        the columns in self.selected_features (order preserved).

        This is a simplified version - only generates basic features.
        For full feature engineering, see train_ppo_feature_combinations.py
        """
        try:
            # Ensure required raw columns exist
            required = {'open', 'high', 'low', 'close', 'volume'}
            missing_required = required - set(df.columns)
            if missing_required:
                logger.warning(f"Missing required columns for {stock}: {missing_required}")
                for col in missing_required:
                    df[col] = 0.0

            df_engineered = pd.DataFrame(index=df.index)

            # Copy any raw columns that are requested and present
            for col in self.selected_features:
                if col in df.columns:
                    df_engineered[col] = df[col]

            # Prepare set of needed (requested but not present)
            needed = [f for f in self.selected_features if f not in df_engineered.columns]

            # === BASIC PRICE FEATURES ===
            # Returns (multiple timeframes)
            periods_map = {'returns_1d':1, 'returns_3d':3, 'returns_5d':5, 'returns_10d':10, 'returns_20d':20}
            for feat, p in periods_map.items():
                if feat in needed and 'close' in df.columns:
                    df_engineered[feat] = df['close'].pct_change(p).fillna(0.0)

            # Log returns
            log_map = {'log_returns_1d':1, 'log_returns_5d':5}
            for feat, p in log_map.items():
                if feat in needed and 'close' in df.columns:
                    df_engineered[feat] = np.log(df['close'] / df['close'].shift(p)).fillna(0.0)

            # Gap and range features
            if 'overnight_gap' in needed and has_columns(df, ['open', 'close']):
                df_engineered['overnight_gap'] = (df['open'] / df['close'].shift(1) - 1.0).fillna(0.0)

            if 'daily_range' in needed and has_columns(df, ['high', 'low', 'open']):
                df_engineered['daily_range'] = ((df['high'] - df['low']) / df['open'].replace(0, np.nan)).fillna(0.0)

            if 'close_position' in needed and has_columns(df, ['high', 'low', 'close']):
                denom = df['high'] - df['low']
                df_engineered['close_position'] = ((df['close'] - df['low']) / denom.replace(0, np.nan)).fillna(0.0)

            # Fill any requested features that still don't exist with zeros
            for feat in self.selected_features:
                if feat not in df_engineered.columns:
                    logger.warning(f"Feature '{feat}' could not be generated for {stock}, filling with zeros")
                    df_engineered[feat] = 0.0

            # Final enforcement: reorder columns to match selected_features exactly
            df_engineered = df_engineered[self.selected_features]

            logger.info(f"CustomDataLoader: Generated {len(df_engineered.columns)} features for {stock}")
            return df_engineered.fillna(0.0)

        except Exception as e:
            logger.error(f"Error in CustomDataLoader.engineer_features for {stock}: {e}", exc_info=True)
            # Return DataFrame of zeros with selected features
            return pd.DataFrame(0.0, index=df.index, columns=self.selected_features)


# -----------------------
# CustomPortfolioEnv
# -----------------------
class CustomPortfolioEnv(JAXVectorizedPortfolioEnv):
    """
    Vectorized portfolio environment that uses CustomDataLoader to ensure the requested
    features are used end-to-end.

    CRITICAL: We override __init__ completely to prevent parent from creating its own loader.
    """
    def __init__(self, selected_features: List[str],
                data_root: str = 'processed_data/',
                stocks: List[str] = None,
                initial_cash: float = 1_000_000.0,
                window_size: int = 30,
                start_date: str = '2024-06-06',
                end_date: str = '2025-03-06',
                transaction_cost_rate: float = 0.005,
                sharpe_window: int = 252,
                **kwargs):
        """
        Initialize CustomPortfolioEnv with complete control over feature selection.

        We DON'T call super().__init__() because the parent creates its own JAXPortfolioDataLoader
        which bypasses our custom loader. Instead, we replicate the parent's init logic
        but use our CustomDataLoader.
        """
        try:
            # Store and normalize selected features FIRST
            self.selected_features = [str(f) for f in selected_features]
            self.features = list(self.selected_features)
            self.n_features = len(self.selected_features)
            self.use_all_features = False

            logger.info(f"CustomPortfolioEnv: Initializing with {len(self.selected_features)} features")

            # === REPLICATE PARENT'S __init__ LOGIC ===
            self.data_root = data_root
            self.window_size = window_size
            self.start_date = start_date
            self.end_date = end_date
            self.initial_cash_actual = initial_cash
            self.transaction_cost_rate = transaction_cost_rate
            self.sharpe_window = sharpe_window
            self.risk_free_rate_daily = 0.04 / 252.0
            self.cash_return_rate = 0.04 / 252.0
            self.close_price_idx = None

            # Load stocks
            if stocks is None:
                stocks = self._load_stock_list()
            self.stocks = stocks
            self.n_stocks = len(stocks)

            logger.info(f"CustomPortfolioEnv: Data root: {self.data_root}")
            logger.info(f"CustomPortfolioEnv: Stocks loaded: {len(self.stocks)}")

            # === CRITICAL: Use CustomDataLoader instead of parent's JAXPortfolioDataLoader ===
            self.data_loader = CustomDataLoader(
                selected_features=self.selected_features,
                data_root=data_root,
                stocks=stocks,
                use_all_features=False
            )

            # Load data using our custom loader
            logger.info(f"CustomPortfolioEnv: Loading data with {len(self.selected_features)} features...")
            self.data, self.dates_idx, self.actual_dates, self.n_features = self.data_loader.load_and_preprocess_data(
                start_date=start_date,
                end_date=end_date,
                preload_to_gpu=True,
                force_reload=False  # Use cache when available
            )

            # Verify data shape
            if self.data.shape[2] != len(self.selected_features):
                raise ValueError(
                    f"Data shape mismatch! Expected {len(self.selected_features)} features, "
                    f"got {self.data.shape[2]}. Data shape: {self.data.shape}"
                )

            logger.info(f"CustomPortfolioEnv: Data loaded successfully with shape: {self.data.shape}")

            # Set close price index
            self.close_price_idx = 0  # Always 0 after reorganization

            # Create feature indices dict
            self.feature_indices = {
                'open': self.data_loader.features.index('open') if 'open' in self.data_loader.features else 0,
                'close': self.data_loader.features.index('close') if 'close' in self.data_loader.features else 0,
                'returns_1d': self.data_loader.features.index('returns_1d') if 'returns_1d' in self.data_loader.features else None,
                'volatility_10d': self.data_loader.features.index('volatility_10d') if 'volatility_10d' in self.data_loader.features else None,
                'overnight_gap': self.data_loader.features.index('overnight_gap') if 'overnight_gap' in self.data_loader.features else None
            }

            # Validate data
            if self.data.shape[0] < self.window_size + 2:
                raise ValueError(
                    f"Insufficient data: need at least {self.window_size + 2} time steps, "
                    f"got {self.data.shape[0]}"
                )
            self.n_timesteps = len(self.dates_idx)

            # Set action and observation dimensions
            self.action_dim = self.n_stocks + 1

            # Calculate observation dimension
            obs_size = (
                (self.window_size * self.n_stocks * self.n_features) +  # Historical features
                self.n_stocks * 2 +                                     # Current open prices + gaps
                self.action_dim +                                       # Portfolio weights
                self.n_stocks +                                         # Short position flags
                8                                                       # Market state
            )
            self.obs_dim = obs_size

            logger.info(f"CustomPortfolioEnv: Initialized successfully")
            logger.info(f"  Stocks: {self.n_stocks}")
            logger.info(f"  Features: {self.n_features}")
            logger.info(f"  Window size: {self.window_size}")
            logger.info(f"  Observation dim: {self.obs_dim}")
            logger.info(f"  Action dim: {self.action_dim}")
            logger.info(f"  Timesteps: {self.n_timesteps}")

        except Exception as e:
            logger.error(f"Error in CustomPortfolioEnv initialization: {e}", exc_info=True)
            raise

    def _load_stock_list(self) -> List[str]:
        """Load stock list from file or scan directory"""
        from pathlib import Path
        stocks_file = Path("finagent/stocks.txt")
        if not stocks_file.exists():
            stocks_file = Path("FYP-FinAgent/finagent/stocks.txt")

        if stocks_file.exists():
            with open(stocks_file, 'r') as f:
                stocks = [line.strip() for line in f.readlines() if line.strip()]
                logger.info(f"Loaded {len(stocks)} stocks from file")
                return stocks

        # Fallback: scan directory
        from pathlib import Path
        data_path = Path(self.data_root) if isinstance(self.data_root, str) else self.data_root
        stocks = [p.stem.replace('_aligned', '') for p in data_path.glob("*_aligned.csv")]
        logger.info(f"Scanned {len(stocks)} stocks from directory")
        return stocks


class EpisodeData(NamedTuple):
    """Data structure for storing episode information"""
    obs: chex.Array
    actions: chex.Array
    rewards: chex.Array
    log_probs: chex.Array
    values: chex.Array
    lstm_carry_h: chex.Array
    lstm_carry_c: chex.Array

# Curriculum configuration class
class CurriculumConfig:
    def __init__(self):
        # Define curriculum stages here (updated for longer training)
        self.stages = {
            1: {"stage_num": 1, "name": "Exploration", "description": "Initial exploration stage",
                "learning_rate": 4e-5, "action_std": 0.18, "entropy_coeff": 0.007,
                "num_updates": 800,  # Increased from 1000
                "early_stopping_patience": 500,  # Increased from default
                "early_stopping_min_delta": 0.001},
            2: {"stage_num": 2, "name": "Refinement", "description": "Refinement of learned policies",
                "learning_rate": 2e-5, "action_std": 0.15, "entropy_coeff": 0.005,
                "num_updates": 1000,  # Increased from 800
                "early_stopping_patience": 600,
                "early_stopping_min_delta": 0.003},
            3: {"stage_num": 3, "name": "Optimization", "description": "Final optimization stage",
                "learning_rate": 1e-5, "action_std": 0.12, "entropy_coeff": 0.003,
                "num_updates": 800,  # Increased from 600
                "early_stopping_patience": 700,
                "early_stopping_min_delta": 0.005},
        }

    def get_stage(self, stage_num):
        return self.stages.get(stage_num, None)


class PlainRLLSTMTrainer:
    """Plain RL Trainer using REINFORCE with LSTM architecture and feature combinations"""
    
    def __init__(self, config: Dict[str, Any], selected_features: List[str], curriculum_stage: int = None, curriculum_config: CurriculumConfig = None):
        """
        Initialize trainer with plain RL algorithm and LSTM architecture with feature selection.
        
        Args:
            config: Training configuration
            selected_features: List of features to use for training
        """
        self.selected_features = selected_features
        
        # Update config to use custom environment
        config['use_custom_env'] = True
        config['selected_features'] = selected_features
        
        logger.info(f"Initializing Plain RL LSTM Trainer with {len(selected_features)} features")
        
        self.config = config
        self.curriculum_stage = curriculum_stage
        self.curriculum_config = curriculum_config
        self.nan_count = 0
        self.max_nan_resets = 5
        
        # Early stopping configuration
        self.early_stopping_patience = config.get('early_stopping_patience', 200)
        self.early_stopping_min_delta = config.get('early_stopping_min_delta', 0.001)
        self.best_performance = -1e10
        self.patience_counter = 0
        self.performance_history = []
        self.should_stop_early = False
        self.global_step = 0
        self.current_raw_rewards = None

        # Apply curriculum stage settings if provided
        if self.curriculum_stage and self.curriculum_config:
            self._apply_curriculum_settings()

        logger.info("Initializing Plain RL LSTM Trainer...")

        # Initialize environment with error handling
        try:
            self._initialize_environment()
        except Exception as e:
            logger.error(f"Failed to initialize environment: {e}")
            raise

        # Vectorized environment functions
        self.vmap_reset = jax.vmap(self.env.reset, in_axes=(0,))
        self.vmap_step = jax.vmap(self.env.step, in_axes=(0, 0))
        
        # Initialize network
        self.network = ActorCriticLSTM(
            action_dim=self.env.action_dim,
            hidden_size=config.get('hidden_size', 256),
            n_lstm_layers=config.get('n_lstm_layers', 1)
        )

        # Initialize parameters with robust error handling
        self._initialize_parameters()

        # Setup optimizer for REINFORCE
        self.optimizer = optax.adam(
            learning_rate=config.get('learning_rate', 1e-4),
            eps=1e-8,
            b1=0.9,
            b2=0.999
        )

        # Create training state
        self.train_state = train_state.TrainState.create(
            apply_fn=self.network.apply,
            params=self.params,
            tx=self.optimizer
        )

        # Initialize RNG
        self.rng = jax.random.PRNGKey(self.config.get('seed', 42))
        
        # Initialize environment and LSTM state
        self._initialize_environment_state()

        # Initialize wandb if enabled
        if config.get('use_wandb', False) and wandb is not None:
            try:
                wandb.init(
                    project=config.get('wandb_project', 'finagent-plain-rl-lstm'),
                    config=config,
                    name=f"plain-rl-lstm-{int(time.time())}"
                )
                logger.info("Wandb initialized successfully")
            except Exception as e:
                logger.warning(f"Failed to initialize wandb: {e}")
        elif config.get('use_wandb', False) and wandb is None:
            logger.warning("Wandb requested but not available. Install wandb to enable logging.")

        # Initialize balanced learning components
        self.training_step = 0
        self.portfolio_values_history = []
        self.returns_history = []
        
        # Reward normalization
        self.reward_normalizer = self._create_reward_normalizer()
        
        # Learning rate scheduler
        self.lr_scheduler = self._create_lr_scheduler()
        
        logger.info("Plain RL LSTM Trainer initialization complete!")
    
    def _create_reward_normalizer(self):
        """Create reward normalizer for stable learning"""
        clip_range = self.config.get('reward_clip_range', (-8.0, 8.0))
        scale = self.config.get('reward_scale', 1.2)
        
        class RewardNormalizer:
            def __init__(self, clip_range, scale, window_size=100):
                self.clip_range = clip_range
                self.scale = scale
                self.window_size = window_size
                self.reward_history = []
                self.running_mean = 0.0
                self.running_std = 1.0
                self.alpha = 0.01  # Exponential moving average factor
            
            def normalize(self, rewards):
                """Normalize and clip rewards"""
                rewards = np.array(rewards)
                
                # Update running statistics
                if len(self.reward_history) > 0:
                    self.running_mean = (1 - self.alpha) * self.running_mean + self.alpha * rewards.mean()
                    self.running_std = (1 - self.alpha) * self.running_std + self.alpha * rewards.std()
                
                # Add to history
                self.reward_history.extend(rewards.flatten())
                if len(self.reward_history) > self.window_size:
                    self.reward_history = self.reward_history[-self.window_size:]
                
                # Normalize rewards
                if self.running_std > 1e-8:
                    normalized_rewards = (rewards - self.running_mean) / self.running_std
                else:
                    normalized_rewards = rewards
                
                # Scale and clip
                scaled_rewards = normalized_rewards * self.scale
                clipped_rewards = np.clip(scaled_rewards, self.clip_range[0], self.clip_range[1])
                
                return clipped_rewards
        
        return RewardNormalizer(clip_range, scale)
    
    def _create_lr_scheduler(self):
        """Create learning rate scheduler for gradual learning"""
        initial_lr = self.config.get('learning_rate', 4e-5)
        schedule_type = self.config.get('lr_schedule_type', 'cosine')
        decay_steps = self.config.get('lr_decay_steps', 6000)
        min_lr = self.config.get('lr_min', 8e-7)
        
        class LearningRateScheduler:
            def __init__(self, initial_lr, schedule_type, decay_steps, min_lr):
                self.initial_lr = initial_lr
                self.schedule_type = schedule_type
                self.decay_steps = decay_steps
                self.min_lr = min_lr
            
            def get_lr(self, step):
                """Get learning rate for current step"""
                if self.schedule_type == 'cosine':
                    # Cosine annealing
                    progress = min(step / self.decay_steps, 1.0)
                    lr = self.min_lr + (self.initial_lr - self.min_lr) * 0.5 * (1 + np.cos(np.pi * progress))
                elif self.schedule_type == 'linear':
                    # Linear decay
                    progress = min(step / self.decay_steps, 1.0)
                    lr = self.initial_lr - (self.initial_lr - self.min_lr) * progress
                elif self.schedule_type == 'exponential':
                    # Exponential decay
                    decay_rate = np.log(self.min_lr / self.initial_lr) / self.decay_steps
                    lr = self.initial_lr * np.exp(decay_rate * step)
                else:
                    lr = self.initial_lr
                
                return max(lr, self.min_lr)
        
        return LearningRateScheduler(initial_lr, schedule_type, decay_steps, min_lr)
    
    def _get_env_config(self) -> Dict[str, Any]:
        """Get environment configuration"""
        return {
            'data_root': self.config['data_root'],
            'stocks': self.config.get('stocks', None),
            'features': self.config.get('features', None),
            'initial_cash': self.config.get('initial_cash', 1000000.0),
            'window_size': self.config.get('window_size', 30),
            'start_date': self.config['train_start_date'],
            'end_date': self.config['train_end_date'],
            'transaction_cost_rate': self.config.get('transaction_cost_rate', 0.005),
            'sharpe_window': self.config.get('sharpe_window', 252),
            'use_all_features': self.config.get('use_all_features', True)
        }
    
    def _initialize_environment(self):
        """Initialize custom environment with feature selection"""
        try:
            env_config = self._get_env_config()
            
            # Use custom environment class with selected features
            self.env = CustomPortfolioEnv(self.selected_features, **env_config)
            logger.info(f"Custom environment initialized: obs_dim={self.env.obs_dim}, action_dim={self.env.action_dim}")
            logger.info(f"Using {len(self.selected_features)} features: {self.selected_features[:5]}...")
            
        except Exception as e:
            logger.error(f"Failed to initialize custom environment: {e}")
            raise
    
    def _initialize_parameters(self):
        """Initialize network parameters"""
        logger.info("Initializing network parameters...")
        
        # Initialize RNG
        self.rng = jax.random.PRNGKey(self.config.get('seed', 42))
        self.rng, init_rng = jax.random.split(self.rng)

        # Create dummy inputs for initialization
        dummy_obs = jnp.ones((self.config.get('n_envs', 8), self.env.obs_dim))
        dummy_carry = self._create_dummy_carry(self.config.get('n_envs', 8))

        # Initialize parameters
        try:
            self.params = self.network.init(init_rng, dummy_obs, dummy_carry)
            logger.info("Network parameters initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize network parameters: {e}")
            raise

    def _apply_curriculum_settings(self):
        """Apply curriculum-specific settings to the configuration."""
        stage = self.curriculum_config.get_stage(self.curriculum_stage)
        if not stage:
            raise ValueError(f"Invalid curriculum stage: {self.curriculum_stage}")

        logger.info(f"Applying curriculum stage {stage['stage_num']}: {stage['name']}")
        logger.info(f"Description: {stage['description']}")

        # Update config with stage-specific hyperparameters
        self.config.update({
            'learning_rate': stage['learning_rate'],
            'action_std': stage['action_std'],
            'entropy_coeff': stage['entropy_coeff'],
            'num_updates': stage['num_updates'],
        })

        # Early stopping settings from curriculum config
        early_stopping_patience = stage.get('early_stopping_patience', 500)
        early_stopping_min_delta = stage.get('early_stopping_min_delta', 0.001)
        self.config.update({
            'early_stopping_patience': early_stopping_patience,
            'early_stopping_min_delta': early_stopping_min_delta
        })
        self.early_stopping_patience = early_stopping_patience
        self.early_stopping_min_delta = early_stopping_min_delta
        logger.info(f"Early stopping: patience={early_stopping_patience}, min_delta={early_stopping_min_delta}")

    def _create_dummy_carry(self, batch_size: int) -> List[LSTMState]:
        """Create dummy LSTM carry states"""
        return [
            LSTMState(
                h=jnp.zeros((batch_size, self.config.get('hidden_size', 256))),
                c=jnp.zeros((batch_size, self.config.get('hidden_size', 256)))
            ) for _ in range(self.config.get('n_lstm_layers', 1))
        ]

    def _initialize_environment_state(self):
        """Initialize environment and LSTM states"""
        logger.info("Initializing environment state...")

        try:
            # Initialize environment
            self.rng, *reset_keys = random.split(self.rng, self.config.get('n_envs', 8) + 1)
            reset_keys = jnp.array(reset_keys)
            self.env_states, self.obs = self.vmap_reset(reset_keys)

            # Clean environment observations (handle inf/nan)
            self.obs = jnp.where(jnp.isnan(self.obs), 0.0, self.obs)
            self.obs = jnp.where(jnp.isinf(self.obs), 0.0, self.obs)
            
            # Initialize LSTM carry states
            self.collector_carry = self._create_dummy_carry(self.config.get('n_envs', 8))

            logger.info("✅ Environment state initialized successfully")

        except Exception as e:
            logger.error(f"Failed to initialize environment state: {e}")
            raise

    def collect_episode(self, train_state: train_state.TrainState, env_states: List[EnvState],
                       initial_obs: chex.Array, initial_carry: List[LSTMState],
                       rng_key: chex.PRNGKey) -> Tuple[EpisodeData, List[EnvState], chex.Array, List[LSTMState]]:
        """Collect a complete episode using current policy"""
        
        def step_fn(carry_step, _):
            """Single step in episode collection"""
            env_states, obs, lstm_carry, rng_key = carry_step

            # Clean observations
            obs = jnp.where(jnp.isnan(obs), 0.0, obs)
            obs = jnp.clip(obs, -50.0, 50.0)
        
            # Get action from policy
            rng_key, action_rng = random.split(rng_key)
        
            # Apply network
            logits, values, new_carry = train_state.apply_fn(
                train_state.params, obs, lstm_carry
            )

            # Clean network outputs
            logits = jnp.where(jnp.isnan(logits), 0.0, logits)
            values = jnp.where(jnp.isnan(values), 0.0, values)
            logits = jnp.clip(logits, -5.0, 5.0)
            values = jnp.clip(values, -50.0, 50.0)

            # Sample actions using REINFORCE (no clipping like PPO)
            action_std = self.config.get('action_std', 1.0)
            action_std = jnp.maximum(action_std, 1e-6)
            
            action_distribution = distrax.Normal(loc=logits, scale=action_std)
            actions = action_distribution.sample(seed=action_rng)
            actions = jnp.clip(actions, -5.0, 5.0)
            
            # Calculate log probabilities
            log_probs = action_distribution.log_prob(actions).sum(axis=-1)
            log_probs = jnp.where(jnp.isnan(log_probs), -10.0, log_probs)
            log_probs = jnp.clip(log_probs, -50.0, 10.0)
        
            # Step environment
            new_env_states, next_obs, rewards, dones, info = self.vmap_step(env_states, actions)

            # Clean environment outputs
            next_obs = jnp.where(jnp.isnan(next_obs), 0.0, next_obs)
            next_obs = jnp.clip(next_obs, -50.0, 50.0)
            
            rewards = jnp.where(jnp.isnan(rewards), 0.0, rewards)
            rewards = jnp.clip(rewards, -50.0, 50.0)

            # Handle LSTM state resets on episode boundaries
            reset_carry = []
            for i, layer_carry in enumerate(new_carry):
                # Clean carry state first
                layer_carry = LSTMState(
                    h=jnp.where(jnp.isnan(layer_carry.h), 0.0, layer_carry.h),
                    c=jnp.where(jnp.isnan(layer_carry.c), 0.0, layer_carry.c)
                )

                # Reset on episode boundaries
                dones_expanded = dones[:, None]
                reset_h = jnp.where(dones_expanded, jnp.zeros_like(layer_carry.h), layer_carry.h)
                reset_c = jnp.where(dones_expanded, jnp.zeros_like(layer_carry.c), layer_carry.c)
                reset_carry.append(LSTMState(h=reset_h, c=reset_c))

            # Stack LSTM carry states for episode storage
            lstm_h_stacked = jnp.stack([c.h for c in lstm_carry], axis=1)
            lstm_c_stacked = jnp.stack([c.c for c in lstm_carry], axis=1)

            # Create episode transition
            transition = EpisodeData(
                obs=obs,
                actions=actions,
                rewards=rewards,
                log_probs=log_probs,
                values=values,
                lstm_carry_h=lstm_h_stacked,
                lstm_carry_c=lstm_c_stacked
            )
        
            return (new_env_states, next_obs, reset_carry, rng_key), transition
        
        # Roll out episode using regular Python loop for stability
        episode_length = self.config.get('episode_length', 100)
        initial_carry = (env_states, initial_obs, initial_carry, rng_key)
        
        try:
            current_carry = initial_carry
            episode_list = []

            for step in range(episode_length):
                current_carry, transition = step_fn(current_carry, None)
                episode_list.append(transition)
            
            # Convert to episode format
            episode = jax.tree_util.tree_map(
                lambda *args: jnp.stack(args, axis=0), *episode_list
            )
            
        except Exception as e:
            # Return safe defaults
            episode = self._create_empty_episode()
            return episode, env_states, initial_obs, initial_carry

        final_env_states, final_obs, final_lstm_carry, _ = current_carry

        return episode, final_env_states, final_obs, final_lstm_carry

    def _create_empty_episode(self) -> EpisodeData:
        """Create empty episode for error recovery"""
        episode_length = self.config.get('episode_length', 100)
        n_envs = self.config.get('n_envs', 8)

        return EpisodeData(
            obs=jnp.zeros((episode_length, n_envs, self.env.obs_dim)),
            actions=jnp.zeros((episode_length, n_envs, self.env.action_dim)),
            rewards=jnp.zeros((episode_length, n_envs)),
            log_probs=jnp.zeros((episode_length, n_envs)),
            values=jnp.zeros((episode_length, n_envs)),
            lstm_carry_h=jnp.zeros((episode_length, n_envs, self.config.get('n_lstm_layers', 1), self.config.get('hidden_size', 256))),
            lstm_carry_c=jnp.zeros((episode_length, n_envs, self.config.get('n_lstm_layers', 1), self.config.get('hidden_size', 256)))
        )

    @partial(jax.jit, static_argnums=(0,))
    def compute_returns(self, episode: EpisodeData) -> chex.Array:
        """Compute discounted returns for REINFORCE"""
        gamma = self.config.get('gamma', 0.99)
        
        # Clean rewards
        rewards = jnp.where(jnp.isnan(episode.rewards), 0.0, episode.rewards)
        rewards = jnp.clip(rewards, -100.0, 100.0)

        # Compute discounted returns
        returns = jnp.zeros_like(rewards)
        running_return = jnp.zeros(rewards.shape[1])  # For each environment
        
        for t in reversed(range(rewards.shape[0])):
            running_return = rewards[t] + gamma * running_return
            returns = returns.at[t].set(running_return)
        
        # Clean returns
        returns = jnp.where(jnp.isnan(returns), 0.0, returns)
        returns = jnp.clip(returns, -100.0, 100.0)
        
        return returns

    @partial(jax.jit, static_argnums=(0,))
    def reinforce_loss(self, params: chex.Array, episode: EpisodeData, returns: chex.Array,
                       rng_key: chex.PRNGKey) -> Tuple[chex.Array, Dict[str, chex.Array]]:
        """Compute REINFORCE loss (policy gradient)"""

        # Clean episode data
        obs_batch = jnp.where(jnp.isnan(episode.obs), 0.0, episode.obs)
        actions_batch = jnp.where(jnp.isnan(episode.actions), 0.0, episode.actions)
        old_log_probs_batch = jnp.where(jnp.isnan(episode.log_probs), -10.0, episode.log_probs)

        # Clean LSTM carry states
        lstm_h_batch = jnp.where(jnp.isnan(episode.lstm_carry_h), 0.0, episode.lstm_carry_h)
        lstm_c_batch = jnp.where(jnp.isnan(episode.lstm_carry_c), 0.0, episode.lstm_carry_c)

        # Reconstruct LSTM carry states
        batch_size = obs_batch.shape[0]
        n_lstm_layers = lstm_h_batch.shape[-2] if len(lstm_h_batch.shape) > 1 else 1

        lstm_carry_batch = []
        for i in range(n_lstm_layers):
            h_state = lstm_h_batch[:, i, :] if len(lstm_h_batch.shape) > 2 else lstm_h_batch
            c_state = lstm_c_batch[:, i, :] if len(lstm_c_batch.shape) > 2 else lstm_c_batch
            lstm_carry_batch.append(LSTMState(h=h_state, c=c_state))
        
        # Get current policy outputs
        logits, values, _ = self.network.apply(params, obs_batch, lstm_carry_batch)

        # Clean network outputs
        logits = jnp.where(jnp.isnan(logits), 0.0, logits)
        values = jnp.where(jnp.isnan(values), 0.0, values)
        logits = jnp.clip(logits, -10.0, 10.0)
        values = jnp.clip(values, -100.0, 100.0)
        
        # Clean targets
        returns = jnp.where(jnp.isnan(returns), 0.0, returns)
        returns = jnp.clip(returns, -100.0, 100.0)

        # REINFORCE policy loss (no clipping like PPO)
        action_std = self.config.get('action_std', 1.0)
        action_std = jnp.maximum(action_std, 1e-6)

        action_distribution = distrax.Normal(loc=logits, scale=action_std)
        new_log_probs = action_distribution.log_prob(actions_batch).sum(axis=-1)
        new_log_probs = jnp.where(jnp.isnan(new_log_probs), -10.0, new_log_probs)
        new_log_probs = jnp.clip(new_log_probs, -50.0, 10.0)
        
        # REINFORCE with baseline (Actor-Critic style)
        # Using value function as baseline significantly reduces variance
        advantages = returns - values  # This is the key improvement!
        advantages = jnp.where(jnp.isnan(advantages), 0.0, advantages)
        advantages = jnp.clip(advantages, -100.0, 100.0)

        # Optionally normalize advantages for stability
        if self.config.get('normalize_advantages', True):
            adv_mean = jnp.mean(advantages)
            adv_std = jnp.std(advantages)
            adv_std = jnp.maximum(adv_std, 1e-8)
            advantages = (advantages - adv_mean) / adv_std
            advantages = jnp.clip(advantages, -10.0, 10.0)

        # Policy loss: -log_prob * advantage (not return!)
        policy_loss = -(new_log_probs * advantages).mean()

        # Value loss: learn to predict returns accurately
        value_loss = 0.5 * jnp.square(values - returns).mean()
        
        # Entropy bonus
        entropy = action_distribution.entropy().sum(axis=-1)
        entropy = jnp.where(jnp.isnan(entropy), 0.0, entropy)
        entropy = jnp.clip(entropy, 0.0, 10.0)
        entropy_coeff = self.config.get('entropy_coeff', 0.01)
        entropy_loss = -entropy_coeff * entropy.mean()
        
        # Total loss
        value_coeff = self.config.get('value_coeff', 0.5)
        total_loss = policy_loss + value_coeff * value_loss + entropy_loss
        total_loss = jnp.where(jnp.isnan(total_loss), 0.0, total_loss)
        
        # Compute metrics
        metrics = {
            'total_loss': total_loss,
            'policy_loss': policy_loss,
            'value_loss': value_loss,
            'entropy_loss': entropy_loss,
            'mean_return': returns.mean(),
            'mean_advantage': advantages.mean(),
            'std_advantage': jnp.std(advantages),
            'mean_value': values.mean(),
            'mean_log_prob': new_log_probs.mean()
        }
        
        return total_loss, metrics

    def train_step(self, train_state: train_state.TrainState, episode: EpisodeData,
                   rng_key: chex.PRNGKey) -> Tuple[train_state.TrainState, Dict[str, chex.Array]]:
        """Perform one REINFORCE training step with balanced learning features"""

        # Update hyperparameters based on training step
        self.training_step += 1
        
        # Update learning rate
        current_lr = self.lr_scheduler.get_lr(self.training_step)
        
        # Update action standard deviation (warmup)
        warmup_steps = self.config.get('action_std_warmup_steps', 1200)
        action_std_initial = self.config.get('action_std', 0.18)
        action_std_final = self.config.get('action_std_final', 0.12)
        
        if self.training_step < warmup_steps:
            progress = self.training_step / warmup_steps
            current_action_std = action_std_initial + (action_std_final - action_std_initial) * progress
        else:
            current_action_std = action_std_final
        
        # Update entropy coefficient (decay)
        entropy_decay_steps = self.config.get('entropy_decay_steps', 4000)
        entropy_initial = self.config.get('entropy_coeff', 0.007)
        entropy_final = self.config.get('entropy_final', 0.0015)
        
        if self.training_step < entropy_decay_steps:
            progress = self.training_step / entropy_decay_steps
            current_entropy_coeff = entropy_initial + (entropy_final - entropy_initial) * progress
        else:
            current_entropy_coeff = entropy_final
        
        # Update config with current values
        self.config['action_std'] = current_action_std
        self.config['entropy_coeff'] = current_entropy_coeff

        # Store raw rewards BEFORE normalization for metrics computation
        self.current_raw_rewards = episode.rewards

        # Normalize rewards if enabled (DISABLED by default for financial RL)
        # Reason: Normalized rewards lose the signal about actual returns
        # The agent should learn to maximize real portfolio value, not normalized scores
        if self.config.get('reward_normalization', False):  # Changed default to False
            normalized_rewards = self.reward_normalizer.normalize(episode.rewards)
            self.current_normalized_rewards = normalized_rewards
            episode = episode._replace(rewards=normalized_rewards)
            logger.info(f"Reward normalization active: raw mean={self.current_raw_rewards.mean():.4f}, "
                       f"normalized mean={normalized_rewards.mean():.4f}")
        else:
            self.current_normalized_rewards = episode.rewards
        
        # Compute returns
        returns = self.compute_returns(episode)
        
        # Flatten episode data
        flat_episode = jax.tree_util.tree_map(
            lambda x: x.reshape(-1, *x.shape[2:]), episode
        )
        flat_returns = returns.reshape(-1)
        
        # Compute gradients and update
        grad_fn = jax.value_and_grad(self.reinforce_loss, has_aux=True)
        (loss, metrics), grads = grad_fn(
            train_state.params, flat_episode, flat_returns, rng_key
        )

        # Clean gradients
        grads = jax.tree_util.tree_map(
            lambda g: jnp.where(jnp.isnan(g), 0.0, g), grads
        )
        grads = jax.tree_util.tree_map(
            lambda g: jnp.where(jnp.isinf(g), jnp.sign(g) * 10.0, g), grads
        )
        
        # Gradient clipping with balanced learning norm
        grad_clip_norm = self.config.get('grad_clip_norm', 6.0)
        grads = jax.tree_util.tree_map(
            lambda g: jnp.clip(g, -grad_clip_norm, grad_clip_norm), grads
        )

        # Apply gradients
        train_state = train_state.apply_gradients(grads=grads)
        
        # Track portfolio values and returns for robust metrics
        portfolio_values = self.env_states.portfolio_value
        self.portfolio_values_history.extend(portfolio_values.tolist())
        
        # Calculate returns from portfolio values
        if len(self.portfolio_values_history) > 1:
            portfolio_values_array = np.array(self.portfolio_values_history)
            returns = np.diff(portfolio_values_array) / portfolio_values_array[:-1]
            self.returns_history.extend(returns.tolist())
        
        # Add hyperparameter tracking to metrics
        metrics.update({
            'learning_rate': current_lr,
            'action_std': current_action_std,
            'entropy_coeff': current_entropy_coeff,
            'reward_mean': episode.rewards.mean(),
            'reward_std': episode.rewards.std(),
        })

        return train_state, metrics

    def compute_robust_metrics(self, rewards: chex.Array) -> Dict[str, float]:
        """Compute simple, robust metrics like Sharpe ratio from RAW rewards.

        CRITICAL: This should be called with RAW rewards (before normalization)
        not normalized rewards which would make Sharpe ratio always ≈0.
        """
        try:
            # Ensure rewards is a valid array and not empty
            if rewards is None or (hasattr(rewards, 'size') and rewards.size == 0):
                raise ValueError("Rewards are empty or None")
            rewards = jnp.array(rewards)
            rewards = jnp.where(jnp.isfinite(rewards), rewards, 0.0)

            # Sum rewards per env across time (axis 0 is time dimension)
            if rewards.ndim > 1:
                per_env_return = jnp.sum(rewards, axis=0)
            else:
                per_env_return = rewards

            # Compute mean and std safely
            mean_return = jnp.mean(per_env_return)
            std_return = jnp.std(per_env_return)

            # Safe division for Sharpe ratio
            std_return = jnp.where(std_return > 1e-8, std_return, 1.0)
            sharpe = mean_return / std_return

            return {
                'sharpe_ratio': float(jnp.clip(sharpe, -1e3, 1e3)),
                'avg_reward': float(jnp.mean(rewards)),
                'std_reward': float(std_return),
                'mean_return': float(mean_return)  # Add this for better tracking
            }
        except Exception as e:
            logger.warning(f"Error in compute_robust_metrics: {e}")
            return {
                'sharpe_ratio': 0.0,
                'avg_reward': 0.0,
                'std_reward': 1.0,
                'mean_return': 0.0
            }

    def calculate_robust_metric(self, portfolio_values: np.ndarray, returns: np.ndarray) -> float:
        """Calculate robust performance metric for early stopping"""
        if len(returns) == 0:
            return 0.0
        
        # Calculate Sharpe ratio (risk-adjusted returns)
        if returns.std() > 1e-8:
            sharpe_ratio = returns.mean() / returns.std() * np.sqrt(252)  # Annualized
        else:
            sharpe_ratio = 0.0
        
        # Calculate max drawdown
        if len(portfolio_values) > 0:
            peak = np.maximum.accumulate(portfolio_values)
            drawdown = (portfolio_values - peak) / peak
            max_drawdown = abs(np.min(drawdown))
        else:
            max_drawdown = 0.0
        
        # Calculate volatility-adjusted return
        if returns.std() > 1e-8:
            vol_adjusted_return = returns.mean() / returns.std() * np.sqrt(252)
        else:
            vol_adjusted_return = 0.0
        
        # Return the appropriate metric based on config
        performance_metric = self.config.get('performance_metric', 'sharpe_ratio')
        
        if performance_metric == 'sharpe_ratio':
            return sharpe_ratio
        elif performance_metric == 'max_drawdown':
            return -max_drawdown  # Negative for maximization
        elif performance_metric == 'volatility_adjusted_return':
            return vol_adjusted_return
        else:
            return returns.mean()  # Fallback to mean return

    def check_early_stopping(self, current_performance: float, update_step: int) -> bool:
        """Check if early stopping criteria are met using robust metrics with moving window"""
        # Calculate robust metric if we have sufficient data
        if len(self.returns_history) > 10:
            portfolio_values_array = np.array(self.portfolio_values_history[-100:])  # Last 100 steps
            returns_array = np.array(self.returns_history[-100:])
            robust_metric = self.calculate_robust_metric(portfolio_values_array, returns_array)
        else:
            robust_metric = current_performance

        self.performance_history.append(robust_metric)

        # Keep recent history only (moving window)
        window_size = 100
        if len(self.performance_history) > window_size:
            self.performance_history = self.performance_history[-window_size:]

        # Need minimum history to make decisions
        if len(self.performance_history) < 30:
            logger.info(f"Building performance history: {len(self.performance_history)}/30 steps")
            return False

        # Compare to RECENT best (last 50 steps), not global best
        recent_window = self.performance_history[-50:] if len(self.performance_history) >= 50 else self.performance_history[-30:]
        recent_best = max(recent_window)
        recent_mean = np.mean(recent_window)

        improvement = robust_metric - recent_best

        # Update global best for logging (but don't use for early stopping decision)
        if robust_metric > self.best_performance:
            self.best_performance = robust_metric

        if improvement > self.early_stopping_min_delta:
            self.patience_counter = 0
            performance_metric = self.config.get('performance_metric', 'sharpe_ratio')
            logger.info(f"Improvement detected: {robust_metric:.4f} > recent best: {recent_best:.4f} "
                       f"(improvement: {improvement:.4f})")
        else:
            # Check for positive trend even without beating recent best
            if len(self.performance_history) >= 40:
                recent_20 = self.performance_history[-20:]
                older_20 = self.performance_history[-40:-20]
                recent_trend = np.mean(recent_20)
                older_trend = np.mean(older_20)

                # Reward positive trend (reduce patience counter)
                if recent_trend > older_trend + self.early_stopping_min_delta / 2:
                    self.patience_counter = max(0, self.patience_counter - 1)
                    logger.info(f"Positive trend detected: recent mean {recent_trend:.4f} > "
                               f"older mean {older_trend:.4f} (patience reduced)")
                else:
                    self.patience_counter += 1
            else:
                self.patience_counter += 1

        if self.patience_counter >= self.early_stopping_patience:
            performance_metric = self.config.get('performance_metric', 'sharpe_ratio')
            logger.info(f"Early stopping triggered after {self.patience_counter} steps without improvement")
            logger.info(f"Global Best: {self.best_performance:.4f}, Recent Best: {recent_best:.4f}, "
                       f"Recent Mean: {recent_mean:.4f}, Current: {robust_metric:.4f}")
            return True

        # Log progress every check
        if update_step % 10 == 0:
            performance_metric = self.config.get('performance_metric', 'sharpe_ratio')
            logger.info(f"{performance_metric} - Current: {robust_metric:.4f} | Recent Best: {recent_best:.4f} | "
                       f"Recent Mean: {recent_mean:.4f} | Patience: {self.patience_counter}/{self.early_stopping_patience}")

        return False

    def update_stage_parameters(self, stage_params: Dict[str, Any]):
        """Update trainer parameters for a new curriculum stage."""
        logger.info("Updating trainer parameters for new stage:")
        for key, value in stage_params.items():
            if key in self.config:
                logger.info(f"  {key}: {self.config[key]} -> {value}")
                self.config[key] = value
            if key == "curriculum_stage":
                # Update the curriculum stage tracker
                self.curriculum_stage = value
            if key == "global_step":
                # Track global step across stages (for logging)
                self.global_step = value

        # Update early stopping parameters if provided
        if 'early_stopping_patience' in stage_params:
            self.early_stopping_patience = stage_params['early_stopping_patience']
        if 'early_stopping_min_delta' in stage_params:
            self.early_stopping_min_delta = stage_params['early_stopping_min_delta']

        # Reset stage-specific counters and metrics
        self.patience_counter = 0
        self.best_performance = -1e10
        self.performance_history = []

        # Update optimizer with new learning rate if changed
        if 'learning_rate' in stage_params:
            import optax
            from flax.training import train_state as ts
            new_optimizer = optax.adam(
                learning_rate=stage_params['learning_rate'],
                eps=1e-8,
                b1=0.9,
                b2=0.999
            )
            self.train_state = ts.TrainState.create(
                apply_fn=self.network.apply,
                params=self.train_state.params,  # Preserve parameters
                tx=new_optimizer
            )

            # Update learning rate scheduler with new settings
            self.lr_scheduler = self._create_lr_scheduler()

    def train(self):
        """Main training loop using REINFORCE"""
        logger.info("Starting Plain RL LSTM training with REINFORCE...")

        # Training configuration
        num_updates = self.config.get('num_updates', 1000)
        log_interval = self.config.get('log_interval', 10)
        save_interval = self.config.get('save_interval', 50)

        logger.info(f"Training configuration: {num_updates} updates")
        logger.info(f"Early stopping patience: {self.early_stopping_patience} steps")

        for update in range(num_updates):
            start_time = time.time()

            try:
                # Split RNG for collection and training
                self.rng, collect_rng = random.split(self.rng)
            
                # Collect episode
                episode, self.env_states, self.obs, self.collector_carry = self.collect_episode(
                    self.train_state, self.env_states, self.obs, self.collector_carry, collect_rng
                )

                # Perform REINFORCE training step
                self.rng, train_rng = random.split(self.rng)
                self.train_state, metrics = self.train_step(
                    self.train_state, episode, train_rng
                )
            
                # Logging and early stopping check
                if update % log_interval == 0:
                    elapsed = time.time() - start_time

                    # Compute robust metrics from RAW rewards (before normalization)
                    robust_metrics = self.compute_robust_metrics(self.current_raw_rewards)

                    # Compute episode statistics
                    avg_reward = float(episode.rewards.mean())
                    max_return = float(episode.rewards.sum(axis=0).max())
                    total_loss = float(metrics.get('total_loss', 0.0))
                    policy_loss = float(metrics.get('policy_loss', 0.0))
                    value_loss = float(metrics.get('value_loss', 0.0))
                    mean_return = float(metrics.get('mean_return', 0.0))

                    logger.info(
                        f"Update {update}/{num_updates} | "
                        f"Time: {elapsed:.2f}s | "
                        f"Total Loss: {total_loss:.4f} | "
                        f"Policy Loss: {policy_loss:.4f} | "
                        f"Value Loss: {value_loss:.4f} | "
                        f"Avg Reward: {avg_reward:.4f} | "
                        f"Sharpe: {robust_metrics['sharpe_ratio']:.4f} | "
                        f"Mean Adv: {float(metrics.get('mean_advantage', 0.0)):.4f} | "
                        f"Patience: {self.patience_counter}/{self.early_stopping_patience}"
                    )

                    # Check early stopping using Sharpe ratio from raw rewards
                    if self.check_early_stopping(robust_metrics['sharpe_ratio'], update):
                        logger.info(f"Early stopping triggered at update {update}")
                        logger.info(f"Training completed early after {update} updates")
                        break

                    # Log to wandb if enabled
                    if self.config.get('use_wandb', False) and wandb is not None:
                        try:
                            wandb_log = {
                                "losses/total_loss": total_loss,
                                "losses/policy_loss": policy_loss,
                                "losses/value_loss": value_loss,
                                "losses/entropy_loss": float(metrics.get('entropy_loss', 0.0)),
                                "rollout/avg_reward": avg_reward,
                                "rollout/max_reward": float(episode.rewards.max()),
                                "rollout/min_reward": float(episode.rewards.min()),
                                "rollout/avg_episode_return": float(episode.rewards.sum(axis=0).mean()),
                                "rollout/max_episode_return": max_return,
                                "rollout/mean_return": mean_return,
                                "rollout/avg_portfolio_value": float(self.env_states.portfolio_value.mean()),
                                # Advantage metrics (NEW)
                                "advantages/mean_advantage": float(metrics.get('mean_advantage', 0.0)),
                                "advantages/std_advantage": float(metrics.get('std_advantage', 0.0)),
                                "advantages/mean_value": float(metrics.get('mean_value', 0.0)),
                                # Performance metrics (from RAW rewards)
                                "performance/sharpe_ratio": robust_metrics.get('sharpe_ratio', 0.0),
                                "performance/mean_return": robust_metrics.get('mean_return', 0.0),
                                "performance/std_return": robust_metrics.get('std_reward', 1.0),
                                # Hyperparameters
                                "hyperparams/learning_rate": metrics.get('learning_rate', 0.0),
                                "hyperparams/action_std": metrics.get('action_std', 0.0),
                                "hyperparams/entropy_coeff": metrics.get('entropy_coeff', 0.0),
                                # Early stopping
                                "early_stopping/patience_counter": self.patience_counter,
                                "early_stopping/best_performance": self.best_performance,
                                "global_step": update
                            }
                            wandb.log(wandb_log)
                        except Exception as e:
                            logger.warning(f"Wandb logging failed: {e}")
            
                # Save model periodically
                if update % save_interval == 0 and update > 0:
                    try:
                        self.save_model(f"plain_rl_lstm_update_{update}")
                    except Exception as e:
                        pass

            except Exception as e:
                # Continue training despite errors
                continue

        logger.info("Training complete!")

        # Final cleanup - only finish wandb if NOT in curriculum mode
        if self.config.get('use_wandb', False) and wandb is not None and not self.config.get('in_curriculum_mode', False):
            try:
                wandb.finish()
            except Exception as e:
                logger.warning(f"Wandb cleanup failed: {e}")

    def save_model(self, filename: str):
        """Save model parameters"""
        try:
            save_path = Path(self.config.get('model_dir', 'models')) / filename
            save_path = save_path.with_suffix('.pkl')
            save_path.parent.mkdir(parents=True, exist_ok=True)

            # Save model state
            model_state = {
                'params': self.train_state.params,
                'config': self.config,
                'training_step': getattr(self, 'training_step', 0)
            }

            import pickle
            with open(save_path, 'wb') as f:
                pickle.dump(model_state, f)

            logger.info(f"Model saved to {save_path}")

        except Exception as e:
            logger.error(f"Failed to save model: {e}")


def main():
    """Main function with command line argument parsing"""
    parser = argparse.ArgumentParser(
        description="Train Plain RL with LSTM using REINFORCE algorithm and feature combinations",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python train_plain_rl_lstm.py --feature_combination ohlcv+technical
  python train_plain_rl_lstm.py --feature_combination all --num_updates 500
  python train_plain_rl_lstm.py --feature_combination ohlcv+financial+sentiment --use_wandb
        """
    )
    
    # Feature combination argument
    parser.add_argument(
        '--feature_combination', 
        type=str, 
        default='ohlcv+technical',
        help='Feature combination to use (e.g., ohlcv+technical, all, etc.)'
    )

    # Curriculum learning arguments
    parser.add_argument('--curriculum_stage', type=int, choices=[1, 2, 3],
                        help='Specific curriculum stage to run (1=exploration, 2=refinement, 3=optimization)')
    parser.add_argument('--auto_curriculum', action='store_true',
                        help='Automatically progress through all curriculum stages')
    parser.add_argument('--start_stage', type=int, default=1, choices=[1, 2, 3],
                        help='Stage to start from in auto mode')
    
    # Training configuration arguments - Balanced Learning Defaults
    parser.add_argument('--num_updates', type=int, default=2500, help='Number of training updates')
    parser.add_argument('--learning_rate', type=float, default=4e-5, help='Learning rate (balanced: 4e-5)')
    parser.add_argument('--n_envs', type=int, default=32, help='Number of parallel environments (increased for stability)')
    parser.add_argument('--episode_length', type=int, default=80, help='Episode length (reduced from 120 for faster feedback)')
    parser.add_argument('--hidden_size', type=int, default=128, help='LSTM hidden size (balanced: 128)')
    parser.add_argument('--n_lstm_layers', type=int, default=1, help='Number of LSTM layers (balanced: 1)')
    
    # Data configuration arguments
    parser.add_argument('--data_root', type=str, default='processed_data/', help='Data root directory')
    parser.add_argument('--train_start_date', type=str, default='2024-06-06', help='Training start date')
    parser.add_argument('--train_end_date', type=str, default='2025-03-06', help='Training end date')
    parser.add_argument('--window_size', type=int, default=30, help='Observation window size')
    
    # Early stopping arguments - Balanced Learning Defaults
    parser.add_argument('--early_stopping_patience', type=int, default=180, help='Early stopping patience (balanced: 180)')
    parser.add_argument('--early_stopping_min_delta', type=float, default=0.0025, help='Minimum improvement threshold (balanced: 0.0025)')
    parser.add_argument('--performance_metric', type=str, default='sharpe_ratio', 
                       choices=['sharpe_ratio', 'max_drawdown', 'volatility_adjusted_return'],
                       help='Performance metric for early stopping (balanced: sharpe_ratio)')
    
    # Other arguments
    parser.add_argument('--use_wandb', action='store_true', help='Use Weights & Biases logging')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    parser.add_argument('--model_dir', type=str, default='models', help='Model save directory')
    parser.add_argument('--list_combinations', action='store_true', help='List available feature combinations')
    
    args = parser.parse_args()
    
    # Curriculum learning configuration
    curriculum_config = CurriculumConfig()

    # List available combinations if requested
    if args.list_combinations:
        feature_selector = FeatureSelector()
        feature_selector.print_available_combinations()
        return
    
    # Initialize feature selector
    feature_selector = FeatureSelector()
    
    # Get selected features
    try:
        selected_features = feature_selector.get_features_for_combination(args.feature_combination)
        logger.info(f"Selected {len(selected_features)} features for training")
        
        # Log feature distribution by category
        for category, info in feature_selector.feature_categories.items():
            category_features = [f for f in selected_features if f in info['features']]
            logger.info(f"  {category}: {len(category_features)} features")
            
    except Exception as e:
        logger.error(f"Failed to get features for combination '{args.feature_combination}': {e}")
        feature_selector.print_available_combinations()
        raise
    
    # Create training configuration
    config = {
        # Environment settings
        'seed': args.seed,
        'data_root': args.data_root,
        'stocks': None,  # Will be loaded from stocks.txt
        'train_start_date': args.train_start_date,
        'train_end_date': args.train_end_date,
        'window_size': args.window_size,
        'transaction_cost_rate': 0.005,
        'sharpe_window': 252,
        
        # Data loading settings
        'use_all_features': True,  # Use all available features
        'save_cache': True,
        'cache_format': 'hdf5',
        'force_reload': False,
        'preload_to_gpu': True,
        
        # Training environment
        'n_envs': args.n_envs,
        'episode_length': args.episode_length,
        
        # REINFORCE hyperparameters - Balanced Learning Configuration
        'num_updates': args.num_updates,
        'gamma': 0.975,  # Higher gamma for smoother credit assignment
        'learning_rate': args.learning_rate,
        'value_coeff': 0.5,  # Value coefficient for baseline (increased from 0.25)
        'entropy_coeff': 0.007,  # Moderate entropy early, prevents brittle strategies
        'action_std': 0.18,  # Moderate action std for balanced exploration
        'action_std_final': 0.12,  # Final action std for stable exploitation
        'action_std_warmup_steps': 1200,  # Gradual exploration -> exploitation transition
        'normalize_advantages': True,  # Normalize advantages for stability
        
        # Learning rate scheduling
        'lr_schedule_type': 'cosine',  # Smooth decay for fine-grained stability
        'lr_decay_steps': 6000,  # Gradual decay over training
        'lr_min': 8e-7,  # Minimum LR for stability
        
        # Entropy decay
        'entropy_decay_steps': 4000,  # Slow decay maintains exploration
        'entropy_final': 0.0015,  # Final entropy for exploitation
        
        # Reward normalization (DISABLED by default)
        # With simplified reward function, normalization is not needed
        'reward_normalization': False,  # Disabled for clearer learning signal
        'reward_clip_range': (-8.0, 8.0),  # Clip extreme rewards (if normalization enabled)
        'reward_scale': 1.2,  # Scale factor for normalized rewards (if normalization enabled)
        
        # Gradient clipping
        'grad_clip_norm': 6.0,  # Moderate gradient clipping
        
        # Early stopping configuration
        'early_stopping_patience': args.early_stopping_patience,
        'early_stopping_min_delta': args.early_stopping_min_delta,
        'performance_metric': args.performance_metric,
        
        # Network architecture - Balanced Learning Configuration
        'hidden_size': args.hidden_size,
        'n_lstm_layers': args.n_lstm_layers,
        'dropout_rate': 0.12,  # Dropout for regularization
        
        # Logging and monitoring
        'use_wandb': args.use_wandb,
        'log_interval': 20,
        'save_interval': 100,
        'model_dir': args.model_dir,
    }
    
    # Run training
    try:
        if args.auto_curriculum:
            logger.info("Running FULL CURRICULUM (all stages)")

            # Mark as curriculum mode to prevent wandb.finish() in train()
            config['in_curriculum_mode'] = True

            trainer = None
            global_step = 0
            for stage_num in range(args.start_stage, 4):
                logger.info(f"\n{'='*50}\nStarting Curriculum Stage {stage_num}\n{'='*50}")
                stage = curriculum_config.get_stage(stage_num)
                stage_params = {
                    'learning_rate': stage['learning_rate'],
                    'action_std': stage['action_std'],
                    'entropy_coeff': stage['entropy_coeff'],
                    'num_updates': stage['num_updates'],
                    'early_stopping_patience': stage.get('early_stopping_patience', 500),
                    'early_stopping_min_delta': stage.get('early_stopping_min_delta', 0.001),
                    'curriculum_stage': stage_num,
                    'global_step': global_step
                }

                if trainer is None:
                    # Create trainer for first stage
                    trainer = PlainRLLSTMTrainer(config, selected_features, curriculum_stage=stage_num, curriculum_config=curriculum_config)
                else:
                    # Update existing trainer for new stage
                    trainer.update_stage_parameters(stage_params)

                # Train for this stage
                trainer.train()
                trainer.save_model(f"curriculum_stage_{stage_num}")
                global_step += stage['num_updates']
                logger.info(f"Completed Curriculum Stage {stage_num}. Global step: {global_step}")

            # Finish wandb after all curriculum stages complete
            if config.get('use_wandb', False) and wandb is not None:
                try:
                    wandb.finish()
                    logger.info("Weights & Biases run finished")
                except Exception as e:
                    logger.warning(f"Wandb cleanup failed: {e}")
        else:
            trainer = PlainRLLSTMTrainer(config, selected_features, curriculum_stage=args.curriculum_stage, curriculum_config=curriculum_config)
            trainer.train()

        logger.info("Training completed successfully!")

        # Save final model
        trainer.save_model(f"final_model_plain_rl_lstm_{args.feature_combination.replace('+', '_')}")
        
    except KeyboardInterrupt:
        logger.info("Training interrupted by user")
        
    except Exception as e:
        logger.error(f"Training failed with error: {e}")
        raise


if __name__ == "__main__":
    main()
