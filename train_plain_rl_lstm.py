"""
Plain RL Training Script with LSTM Architecture
Uses REINFORCE (policy gradient) algorithm with LSTM neural network
Same state, action, and reward as portfolio_env

This script implements a simpler RL algorithm than PPO while keeping the LSTM architecture
for temporal modeling of the portfolio environment.
"""

import os
import sys
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any, Set, NamedTuple, Tuple
import time

# Configure JAX to use CPU only (fixes Metal GPU compatibility issues)
os.environ['JAX_PLATFORM_NAME'] = 'cpu'

# Setup logging early
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Fix for JAX 0.6.2 + Flax 0.8.4 compatibility issue
def fix_evaltrace_error():
    """Fix EvalTrace level attribute error in JAX 0.6.2 + Flax 0.8.4"""
    try:
        import flax.core.tracers as tracers
        
        def patched_trace_level(main):
            """Patched version of trace_level that handles missing level attribute"""
            if main:
                if hasattr(main, 'level'):
                    return main.level
                else:
                    return 0
            return float('-inf')
        
        tracers.trace_level = patched_trace_level
        logger.info("Applied monkey patch to fix EvalTrace level attribute error")
    except Exception as e:
        logger.warning(f"Could not apply EvalTrace fix: {e}")

# Apply the fix immediately
fix_evaltrace_error()

import jax
import jax.numpy as jnp
import jax.random as random
import numpy as np
import pandas as pd
from functools import partial
import chex

# Import LSTM architecture and environment
from train_ppo_lstm import ActorCriticLSTM, LSTMState
from finagent.environment.portfolio_env import JAXVectorizedPortfolioEnv, EnvState
import optax
from flax.training import train_state
import distrax

# Optional imports for wandb
try:
    import wandb
except ImportError:
    wandb = None


class FeatureSelector:
    """Feature selector for different feature categories"""
    
    def __init__(self):
        # Define feature categories based on actual CSV data and portfolio_env.py analysis
        self.feature_categories = {
            'ohlcv': {
                'description': 'Basic OHLCV price data and simple derived features',
                'features': [
                    # Basic OHLCV (available in CSV)
                    'open', 'high', 'low', 'close', 'volume', 'vwap',
                    
                    # Simple returns (generated by our custom engineer_features)
                    'returns_1d', 'returns_3d', 'returns_5d', 'returns_10d', 'returns_20d',
                    'log_returns_1d', 'log_returns_5d',
                    
                    # Basic price action features (generated by our custom engineer_features)
                    'overnight_gap', 'daily_range', 'close_position',
                    
                    # Lag features (pre-computed in CSV)
                    'open_lag_1', 'open_lag_2', 'open_lag_3', 'open_lag_5', 'open_lag_10', 'open_lag_20',
                    'high_lag_1', 'high_lag_2', 'high_lag_3', 'high_lag_5', 'high_lag_10', 'high_lag_20',
                    'low_lag_1', 'low_lag_2', 'low_lag_3', 'low_lag_5', 'low_lag_10', 'low_lag_20',
                    'close_lag_1', 'close_lag_2', 'close_lag_3', 'close_lag_5', 'close_lag_10', 'close_lag_20',
                    'volume_lag_1', 'volume_lag_2', 'volume_lag_3', 'volume_lag_5', 'volume_lag_10', 'volume_lag_20',
                    
                    # Rolling statistics (pre-computed in CSV)
                    'open_rolling_mean_5', 'open_rolling_mean_20', 'open_rolling_std_20',
                    'high_rolling_mean_5', 'high_rolling_mean_20', 'high_rolling_std_20',
                    'low_rolling_mean_5', 'low_rolling_mean_20', 'low_rolling_std_20',
                    'close_rolling_mean_5', 'close_rolling_mean_20', 'close_rolling_std_20',
                    'close_momentum_5', 'close_momentum_20',
                    'volume_rolling_mean_5', 'volume_rolling_mean_20', 'volume_rolling_std_20'
                ]
            },
            
            'technical': {
                'description': 'Technical indicators and algorithmic trading signals',
                'features': [
                    # Moving averages (pre-computed in CSV)
                    'dma_50', 'dma_200',
                    'dma_50_lag_1', 'dma_50_lag_2', 'dma_50_lag_3', 'dma_50_lag_5', 'dma_50_lag_10', 'dma_50_lag_20',
                    'dma_200_lag_1', 'dma_200_lag_2', 'dma_200_lag_3', 'dma_200_lag_5', 'dma_200_lag_10', 'dma_200_lag_20',
                    'dma_50_rolling_mean_5', 'dma_50_rolling_mean_20', 'dma_50_rolling_std_20',
                    'dma_200_rolling_mean_5', 'dma_200_rolling_mean_20', 'dma_200_rolling_std_20',
                    'dma_cross', 'dma_distance', 'volume_price_trend',
                    
                    # RSI (pre-computed in CSV)
                    'rsi_14',
                    
                    # Volatility features (will be generated by engineer_features)
                    'volatility_5d', 'volatility_10d', 'volatility_20d', 'volatility_30d', 'volatility_60d',
                    'vol_ratio_short_long', 'vol_ratio_5_20',
                    
                    # Momentum features (will be generated)
                    'momentum_5d', 'momentum_10d', 'momentum_20d', 'momentum_60d',
                    'momentum_acceleration_10d',
                    
                    # Moving average features (will be generated)
                    'ma_convergence', 'ma_trend_strength', 'price_position_20d', 
                    'price_above_ma50', 'price_above_ma200',
                    
                    # RSI signals (will be generated)
                    'rsi_oversold', 'rsi_overbought', 'rsi_bullish_divergence', 'rsi_bearish_divergence',
                    'rsi_zscore_20d',
                    
                    # Bollinger Bands (will be generated)
                    'bb_position', 'bb_squeeze', 'bb_breakout_up', 'bb_breakout_down',
                    
                    # Mean reversion signals (will be generated)
                    'price_zscore_20d', 'price_zscore_60d', 'volume_zscore_20d', 'volume_zscore_60d',
                    'price_deviation_50d', 'price_deviation_200d',
                    'mean_reversion_signal_50d', 'mean_reversion_signal_200d',
                    
                    # Breakout signals (will be generated)
                    'price_breakout_20d', 'price_breakdown_20d', 'volume_breakout_20d', 'volume_spike',
                    
                    # Trend following (will be generated)
                    'ma_cross_bullish', 'ma_cross_bearish',
                    
                    # Volatility regime (will be generated)
                    'high_vol_regime', 'low_vol_regime', 'vol_expansion', 'vol_contraction',
                    
                    # Candlestick patterns (will be generated)
                    'body_ratio', 'upper_wick_ratio', 'lower_wick_ratio',
                    'doji_pattern', 'hammer_pattern', 'shooting_star_pattern',
                    
                    # Volume analysis (will be generated)
                    'volume_price_momentum', 'volume_ratio_5d', 'volume_ratio_20d',
                    'volume_trend_10d', 'volume_confirms_price', 'volume_divergence',
                    
                    # Signal aggregation (will be generated)
                    'bullish_signals', 'bearish_signals', 'net_signal_strength',
                    
                    # Risk-adjusted metrics (will be generated)
                    'risk_adjusted_momentum', 'volume_confirmed_trend',
                    
                    # Regime detection (will be generated)
                    'vol_regime_change', 'trend_regime',
                    
                    # Cross-sectional features (will be generated)
                    'momentum_rank_proxy', 'vol_rank_proxy'
                ]
            },
            
            'financial': {
                'description': 'Financial statement metrics and fundamental analysis',
                'features': [
                    # Core Income Statement Features (from CSV)
                    'metric_Revenue', 'metric_TotalRevenue', 'metric_CostofRevenueTotal', 'metric_GrossProfit',
                    'metric_OperatingIncome', 'metric_NetIncomeBeforeTaxes', 'metric_NetIncomeAfterTaxes',
                    'metric_NetIncome', 'metric_DilutedNetIncome', 'metric_DilutedWeightedAverageShares',
                    'metric_DilutedEPSExcludingExtraOrdItems', 'metric_DPS-CommonStockPrimaryIssue',
                    
                    # Core Balance Sheet Features (from CSV)
                    'metric_Cash', 'metric_ShortTermInvestments', 'metric_CashandShortTermInvestments',
                    'metric_TotalCurrentAssets', 'metric_TotalAssets', 'metric_TotalCurrentLiabilities',
                    'metric_TotalLiabilities', 'metric_TotalEquity', 'metric_TotalCommonSharesOutstanding',
                    
                    # Core Cash Flow Features (from CSV)
                    'metric_CashfromOperatingActivities', 'metric_CapitalExpenditures', 
                    'metric_CashfromInvestingActivities', 'metric_CashfromFinancingActivities',
                    'metric_NetChangeinCash', 'metric_TotalCashDividendsPaid',
                    
                    # Key Financial Metrics (from CSV)
                    'metric_freeCashFlowtrailing12Month', 'metric_freeCashFlowMostRecentFiscalYear',
                    'metric_periodLength', 'metric_periodType',
                    
                    # Additional financial metrics (from CSV)
                    'metric_pPerEExcludingExtraordinaryItemsMostRecentFiscalYear',
                    'metric_currentDividendYieldCommonStockPrimaryIssueLTM',
                    'metric_priceToBookMostRecentFiscalYear',
                    'metric_priceToFreeCashFlowPerShareTrailing12Months',
                    'metric_pPerEBasicExcludingExtraordinaryItemsTTM',
                    'metric_pPerEIncludingExtraordinaryItemsTTM',
                    'metric_returnOnAverageEquityMostRecentFiscalYear',
                    'metric_returnOnInvestmentMostRecentFiscalYear',
                    'metric_netProfitMarginPercentTrailing12Month',
                    'metric_operatingMarginTrailing12Month',
                    'metric_grossMarginTrailing12Month',
                    'metric_currentRatioMostRecentFiscalYear',
                    'metric_quickRatioMostRecentFiscalYear',
                    'metric_totalDebtPerTotalEquityMostRecentFiscalYear',
                    'metric_netInterestCoverageMostRecentFiscalYear',
                    'metric_marketCap',
                    'metric_beta'
                ]
            },
            
            'sentiment': {
                'description': 'News and social media sentiment indicators',
                'features': [
                    # Reddit sentiment features (from CSV)
                    'reddit_title_sentiments_mean', 'reddit_title_sentiments_std',
                    'reddit_body_sentiments', 'reddit_body_sentiments_std',
                    'reddit_score_mean', 'reddit_score_sum', 'reddit_posts_count', 'reddit_comments_sum',
                    
                    # News sentiment features (from CSV)
                    'news_sentiment_mean', 'news_articles_count', 'news_sentiment_std', 'news_sources',
                    
                    # Sentiment features that will be generated by engineer_features
                    'sentiment_momentum_3d', 'sentiment_momentum_5d',
                    'sentiment_extreme_positive', 'sentiment_extreme_negative'
                ]
            }
        }
    
    def get_features_for_combination(self, combination: str) -> List[str]:
        """
        Get list of features for a given combination string.
        
        Args:
            combination: Feature combination string like 'ohlcv+technical' or 'all'
            
        Returns:
            List of feature names
        """
        if combination.lower() == 'all':
            # Return all features from all categories
            all_features = []
            for category_features in self.feature_categories.values():
                all_features.extend(category_features['features'])
            return list(set(all_features))  # Remove duplicates
        
        # Parse combination string
        categories = [cat.strip().lower() for cat in combination.split('+')]
        
        # Validate categories
        valid_categories = set(self.feature_categories.keys())
        invalid_categories = set(categories) - valid_categories
        if invalid_categories:
            raise ValueError(f"Invalid feature categories: {invalid_categories}. "
                           f"Valid categories are: {list(valid_categories)}")
        
        # Combine features from selected categories
        selected_features = []
        for category in categories:
            selected_features.extend(self.feature_categories[category]['features'])
        
        # Remove duplicates and ensure 'close' is always first
        selected_features = list(set(selected_features))
        if 'close' in selected_features:
            selected_features.remove('close')
            selected_features = ['close'] + selected_features
        
        return selected_features
    
    def print_available_combinations(self):
        """Print available feature combinations"""
        print("\n=== Available Feature Categories ===")
        for category, info in self.feature_categories.items():
            print(f"\n{category.upper()}: {info['description']}")
            print(f"  Features ({len(info['features'])}): {', '.join(info['features'][:5])}...")
        
        print(f"\n=== Example Combinations ===")
        print("• ohlcv - Basic price data only")
        print("• technical - Technical indicators only")
        print("• financial - Financial metrics only")
        print("• sentiment - Sentiment data only")
        print("• ohlcv+technical - Price data + technical indicators")
        print("• ohlcv+financial - Price data + financial metrics")
        print("• ohlcv+sentiment - Price data + sentiment")
        print("• technical+financial - Technical + financial indicators")
        print("• technical+sentiment - Technical + sentiment indicators")
        print("• financial+sentiment - Financial + sentiment indicators")
        print("• ohlcv+technical+financial - Price + technical + financial")
        print("• ohlcv+technical+sentiment - Price + technical + sentiment")
        print("• ohlcv+financial+sentiment - Price + financial + sentiment")
        print("• technical+financial+sentiment - All except basic price data")
        print("• all - All available features")


class CustomPortfolioEnv(JAXVectorizedPortfolioEnv):
    """Custom portfolio environment with feature selection capability"""
    
    def __init__(self, selected_features: List[str], **kwargs):
        """
        Initialize environment with selected features only.
        
        Args:
            selected_features: List of features to use for training
            **kwargs: Other environment arguments
        """
        # Store selected features
        self.selected_features = selected_features
        logger.info(f"Initializing environment with {len(selected_features)} selected features")
        logger.info(f"Selected features: {selected_features[:10]}..." if len(selected_features) > 10 else f"Selected features: {selected_features}")
        
        # Initialize parent class
        super().__init__(**kwargs)
        
        # Override features in data loader
        self.data_loader.features = selected_features
        
        # Recalculate observation dimension based on selected features
        self._recalculate_observation_dim()
    
    def _recalculate_observation_dim(self):
        """Recalculate observation dimension based on selected features"""
        self.n_features = len(self.selected_features)
        
        # Updated observation size calculation
        obs_size = (
            (self.window_size * self.n_stocks * self.n_features) +  # Historical features
            self.n_stocks * 2 +                                     # Current open prices + gaps
            self.action_dim +                                       # Portfolio weights
            self.n_stocks +                                         # Short position flags
            8                                                       # Market state (8 elements)
        )
        
        self.obs_dim = obs_size
        
        logger.info(f"Recalculated observation dimension: {self.obs_dim}")
        logger.info(f"Features per stock: {self.n_features}")
        logger.info(f"Historical window features: {self.window_size * self.n_stocks * self.n_features}")
    
    def engineer_features(self, df: pd.DataFrame, stock: str) -> pd.DataFrame:
        """
        Override feature engineering to only generate selected features.
        This avoids generating hundreds of unnecessary features.
        """
        # Start with basic OHLCV columns
        df_engineered = df.copy()
        
        # Only generate features that are in our selected_features list
        # This prevents generating hundreds of unnecessary features
        
        # Basic returns (if requested)
        if 'returns_1d' in self.selected_features:
            df_engineered['returns_1d'] = df['close'].pct_change()
        if 'returns_3d' in self.selected_features:
            df_engineered['returns_3d'] = df['close'].pct_change(periods=3)
        if 'returns_5d' in self.selected_features:
            df_engineered['returns_5d'] = df['close'].pct_change(periods=5)
        if 'returns_10d' in self.selected_features:
            df_engineered['returns_10d'] = df['close'].pct_change(periods=10)
        if 'returns_20d' in self.selected_features:
            df_engineered['returns_20d'] = df['close'].pct_change(periods=20)
        
        # Log returns (if requested)
        if 'log_returns_1d' in self.selected_features:
            df_engineered['log_returns_1d'] = np.log(df['close'] / df['close'].shift(1))
        if 'log_returns_5d' in self.selected_features:
            df_engineered['log_returns_5d'] = np.log(df['close'] / df['close'].shift(5))
        
        # Price action features (if requested)
        if 'overnight_gap' in self.selected_features and all(col in df.columns for col in ['open', 'close']):
            df_engineered['overnight_gap'] = df['open'] / df['close'].shift(1) - 1.0
        if 'daily_range' in self.selected_features and all(col in df.columns for col in ['high', 'low', 'open']):
            df_engineered['daily_range'] = (df['high'] - df['low']) / df['open']
        if 'close_position' in self.selected_features and all(col in df.columns for col in ['high', 'low', 'close']):
            df_engineered['close_position'] = (df['close'] - df['low']) / (df['high'] - df['low'] + 1e-8)
        
        # Now filter to only include features that are in our selected_features list
        # and that actually exist in the dataframe
        available_features = [f for f in self.selected_features if f in df_engineered.columns]
        
        # Create filtered dataframe with only selected features
        filtered_df = df_engineered[available_features].copy()
        
        # Ensure 'close' is always first
        if 'close' in filtered_df.columns:
            cols = ['close'] + [c for c in filtered_df.columns if c != 'close']
            filtered_df = filtered_df[cols]
        
        logger.info(f"Generated {len(filtered_df.columns)} features for {stock}: {list(filtered_df.columns)}")
        
        return filtered_df.fillna(0.0)


class EpisodeData(NamedTuple):
    """Data structure for storing episode information"""
    obs: chex.Array
    actions: chex.Array
    rewards: chex.Array
    log_probs: chex.Array
    values: chex.Array
    lstm_carry_h: chex.Array
    lstm_carry_c: chex.Array


class PlainRLLSTMTrainer:
    """Plain RL Trainer using REINFORCE with LSTM architecture and feature combinations"""
    
    def __init__(self, config: Dict[str, Any], selected_features: List[str]):
        """
        Initialize trainer with plain RL algorithm and LSTM architecture with feature selection.
        
        Args:
            config: Training configuration
            selected_features: List of features to use for training
        """
        self.selected_features = selected_features
        
        # Update config to use custom environment
        config['use_custom_env'] = True
        config['selected_features'] = selected_features
        
        logger.info(f"Initializing Plain RL LSTM Trainer with {len(selected_features)} features")
        
        self.config = config
        self.nan_count = 0
        self.max_nan_resets = 5
        
        # Early stopping configuration
        self.early_stopping_patience = config.get('early_stopping_patience', 200)
        self.early_stopping_min_delta = config.get('early_stopping_min_delta', 0.001)
        self.best_performance = -float('inf')
        self.patience_counter = 0
        self.performance_history = []
        self.should_stop_early = False

        logger.info("Initializing Plain RL LSTM Trainer...")

        # Initialize environment with error handling
        try:
            self._initialize_environment()
        except Exception as e:
            logger.error(f"Failed to initialize environment: {e}")
            raise

        # Vectorized environment functions
        self.vmap_reset = jax.vmap(self.env.reset, in_axes=(0,))
        self.vmap_step = jax.vmap(self.env.step, in_axes=(0, 0))
        
        # Initialize network
        self.network = ActorCriticLSTM(
            action_dim=self.env.action_dim,
            hidden_size=config.get('hidden_size', 256),
            n_lstm_layers=config.get('n_lstm_layers', 1)
        )

        # Initialize parameters with robust error handling
        self._initialize_parameters()

        # Setup optimizer for REINFORCE
        self.optimizer = optax.adam(
            learning_rate=config.get('learning_rate', 1e-4),
            eps=1e-8,
            b1=0.9,
            b2=0.999
        )

        # Create training state
        self.train_state = train_state.TrainState.create(
            apply_fn=self.network.apply,
            params=self.params,
            tx=self.optimizer
        )

        # Initialize RNG
        self.rng = jax.random.PRNGKey(self.config.get('seed', 42))
        
        # Initialize environment and LSTM state
        self._initialize_environment_state()

        # Initialize wandb if enabled
        if config.get('use_wandb', False) and wandb is not None:
            try:
                wandb.init(
                    project=config.get('wandb_project', 'finagent-plain-rl-lstm'),
                    config=config,
                    name=f"plain-rl-lstm-{int(time.time())}"
                )
                logger.info("Wandb initialized successfully")
            except Exception as e:
                logger.warning(f"Failed to initialize wandb: {e}")
        elif config.get('use_wandb', False) and wandb is None:
            logger.warning("Wandb requested but not available. Install wandb to enable logging.")

        logger.info("Plain RL LSTM Trainer initialization complete!")
    
    def _get_env_config(self) -> Dict[str, Any]:
        """Get environment configuration"""
        return {
            'data_root': self.config['data_root'],
            'stocks': self.config.get('stocks', None),
            'features': self.config.get('features', None),
            'initial_cash': self.config.get('initial_cash', 1000000.0),
            'window_size': self.config.get('window_size', 30),
            'start_date': self.config['train_start_date'],
            'end_date': self.config['train_end_date'],
            'transaction_cost_rate': self.config.get('transaction_cost_rate', 0.005),
            'sharpe_window': self.config.get('sharpe_window', 252),
            'use_all_features': self.config.get('use_all_features', True)
        }
    
    def _initialize_environment(self):
        """Initialize custom environment with feature selection"""
        try:
            env_config = self._get_env_config()
            
            # Use custom environment class with selected features
            self.env = CustomPortfolioEnv(self.selected_features, **env_config)
            logger.info(f"Custom environment initialized: obs_dim={self.env.obs_dim}, action_dim={self.env.action_dim}")
            logger.info(f"Using {len(self.selected_features)} features: {self.selected_features[:5]}...")
            
        except Exception as e:
            logger.error(f"Failed to initialize custom environment: {e}")
            raise
    
    def _initialize_parameters(self):
        """Initialize network parameters"""
        logger.info("Initializing network parameters...")
        
        # Initialize RNG
        self.rng = jax.random.PRNGKey(self.config.get('seed', 42))
        self.rng, init_rng = jax.random.split(self.rng)

        # Create dummy inputs for initialization
        dummy_obs = jnp.ones((self.config.get('n_envs', 8), self.env.obs_dim))
        dummy_carry = self._create_dummy_carry(self.config.get('n_envs', 8))

        # Initialize parameters
        try:
            self.params = self.network.init(init_rng, dummy_obs, dummy_carry)
            logger.info("Network parameters initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize network parameters: {e}")
            raise

    def _create_dummy_carry(self, batch_size: int) -> List[LSTMState]:
        """Create dummy LSTM carry states"""
        return [
            LSTMState(
                h=jnp.zeros((batch_size, self.config.get('hidden_size', 256))),
                c=jnp.zeros((batch_size, self.config.get('hidden_size', 256)))
            ) for _ in range(self.config.get('n_lstm_layers', 1))
        ]

    def _initialize_environment_state(self):
        """Initialize environment and LSTM states"""
        logger.info("Initializing environment state...")

        try:
            # Initialize environment
            self.rng, *reset_keys = random.split(self.rng, self.config.get('n_envs', 8) + 1)
            reset_keys = jnp.array(reset_keys)
            self.env_states, self.obs = self.vmap_reset(reset_keys)

            # Clean environment observations (handle inf/nan)
            self.obs = jnp.where(jnp.isnan(self.obs), 0.0, self.obs)
            self.obs = jnp.where(jnp.isinf(self.obs), 0.0, self.obs)
            
            # Initialize LSTM carry states
            self.collector_carry = self._create_dummy_carry(self.config.get('n_envs', 8))

            logger.info("✅ Environment state initialized successfully")

        except Exception as e:
            logger.error(f"Failed to initialize environment state: {e}")
            raise

    def collect_episode(self, train_state: train_state.TrainState, env_states: List[EnvState],
                       initial_obs: chex.Array, initial_carry: List[LSTMState],
                       rng_key: chex.PRNGKey) -> Tuple[EpisodeData, List[EnvState], chex.Array, List[LSTMState]]:
        """Collect a complete episode using current policy"""
        
        def step_fn(carry_step, _):
            """Single step in episode collection"""
            env_states, obs, lstm_carry, rng_key = carry_step

            # Clean observations
            obs = jnp.where(jnp.isnan(obs), 0.0, obs)
            obs = jnp.clip(obs, -50.0, 50.0)
        
            # Get action from policy
            rng_key, action_rng = random.split(rng_key)
        
            # Apply network
            logits, values, new_carry = train_state.apply_fn(
                train_state.params, obs, lstm_carry
            )

            # Clean network outputs
            logits = jnp.where(jnp.isnan(logits), 0.0, logits)
            values = jnp.where(jnp.isnan(values), 0.0, values)
            logits = jnp.clip(logits, -5.0, 5.0)
            values = jnp.clip(values, -50.0, 50.0)

            # Sample actions using REINFORCE (no clipping like PPO)
            action_std = self.config.get('action_std', 1.0)
            action_std = jnp.maximum(action_std, 1e-6)
            
            action_distribution = distrax.Normal(loc=logits, scale=action_std)
            actions = action_distribution.sample(seed=action_rng)
            actions = jnp.clip(actions, -5.0, 5.0)
            
            # Calculate log probabilities
            log_probs = action_distribution.log_prob(actions).sum(axis=-1)
            log_probs = jnp.where(jnp.isnan(log_probs), -10.0, log_probs)
            log_probs = jnp.clip(log_probs, -50.0, 10.0)
        
            # Step environment
            new_env_states, next_obs, rewards, dones, info = self.vmap_step(env_states, actions)

            # Clean environment outputs
            next_obs = jnp.where(jnp.isnan(next_obs), 0.0, next_obs)
            next_obs = jnp.clip(next_obs, -50.0, 50.0)
            
            rewards = jnp.where(jnp.isnan(rewards), 0.0, rewards)
            rewards = jnp.clip(rewards, -50.0, 50.0)

            # Handle LSTM state resets on episode boundaries
            reset_carry = []
            for i, layer_carry in enumerate(new_carry):
                # Clean carry state first
                layer_carry = LSTMState(
                    h=jnp.where(jnp.isnan(layer_carry.h), 0.0, layer_carry.h),
                    c=jnp.where(jnp.isnan(layer_carry.c), 0.0, layer_carry.c)
                )

                # Reset on episode boundaries
                dones_expanded = dones[:, None]
                reset_h = jnp.where(dones_expanded, jnp.zeros_like(layer_carry.h), layer_carry.h)
                reset_c = jnp.where(dones_expanded, jnp.zeros_like(layer_carry.c), layer_carry.c)
                reset_carry.append(LSTMState(h=reset_h, c=reset_c))

            # Stack LSTM carry states for episode storage
            lstm_h_stacked = jnp.stack([c.h for c in lstm_carry], axis=1)
            lstm_c_stacked = jnp.stack([c.c for c in lstm_carry], axis=1)

            # Create episode transition
            transition = EpisodeData(
                obs=obs,
                actions=actions,
                rewards=rewards,
                log_probs=log_probs,
                values=values,
                lstm_carry_h=lstm_h_stacked,
                lstm_carry_c=lstm_c_stacked
            )
        
            return (new_env_states, next_obs, reset_carry, rng_key), transition
        
        # Roll out episode using regular Python loop for stability
        episode_length = self.config.get('episode_length', 100)
        initial_carry = (env_states, initial_obs, initial_carry, rng_key)
        
        try:
            current_carry = initial_carry
            episode_list = []

            for step in range(episode_length):
                current_carry, transition = step_fn(current_carry, None)
                episode_list.append(transition)
            
            # Convert to episode format
            episode = jax.tree_util.tree_map(
                lambda *args: jnp.stack(args, axis=0), *episode_list
            )
            
        except Exception as e:
            # Return safe defaults
            episode = self._create_empty_episode()
            return episode, env_states, initial_obs, initial_carry

        final_env_states, final_obs, final_lstm_carry, _ = current_carry

        return episode, final_env_states, final_obs, final_lstm_carry

    def _create_empty_episode(self) -> EpisodeData:
        """Create empty episode for error recovery"""
        episode_length = self.config.get('episode_length', 100)
        n_envs = self.config.get('n_envs', 8)

        return EpisodeData(
            obs=jnp.zeros((episode_length, n_envs, self.env.obs_dim)),
            actions=jnp.zeros((episode_length, n_envs, self.env.action_dim)),
            rewards=jnp.zeros((episode_length, n_envs)),
            log_probs=jnp.zeros((episode_length, n_envs)),
            values=jnp.zeros((episode_length, n_envs)),
            lstm_carry_h=jnp.zeros((episode_length, n_envs, self.config.get('n_lstm_layers', 1), self.config.get('hidden_size', 256))),
            lstm_carry_c=jnp.zeros((episode_length, n_envs, self.config.get('n_lstm_layers', 1), self.config.get('hidden_size', 256)))
        )

    @partial(jax.jit, static_argnums=(0,))
    def compute_returns(self, episode: EpisodeData) -> chex.Array:
        """Compute discounted returns for REINFORCE"""
        gamma = self.config.get('gamma', 0.99)
        
        # Clean rewards
        rewards = jnp.where(jnp.isnan(episode.rewards), 0.0, episode.rewards)
        rewards = jnp.clip(rewards, -100.0, 100.0)

        # Compute discounted returns
        returns = jnp.zeros_like(rewards)
        running_return = jnp.zeros(rewards.shape[1])  # For each environment
        
        for t in reversed(range(rewards.shape[0])):
            running_return = rewards[t] + gamma * running_return
            returns = returns.at[t].set(running_return)
        
        # Clean returns
        returns = jnp.where(jnp.isnan(returns), 0.0, returns)
        returns = jnp.clip(returns, -100.0, 100.0)
        
        return returns

    @partial(jax.jit, static_argnums=(0,))
    def reinforce_loss(self, params: chex.Array, episode: EpisodeData, returns: chex.Array,
                       rng_key: chex.PRNGKey) -> Tuple[chex.Array, Dict[str, chex.Array]]:
        """Compute REINFORCE loss (policy gradient)"""

        # Clean episode data
        obs_batch = jnp.where(jnp.isnan(episode.obs), 0.0, episode.obs)
        actions_batch = jnp.where(jnp.isnan(episode.actions), 0.0, episode.actions)
        old_log_probs_batch = jnp.where(jnp.isnan(episode.log_probs), -10.0, episode.log_probs)

        # Clean LSTM carry states
        lstm_h_batch = jnp.where(jnp.isnan(episode.lstm_carry_h), 0.0, episode.lstm_carry_h)
        lstm_c_batch = jnp.where(jnp.isnan(episode.lstm_carry_c), 0.0, episode.lstm_carry_c)

        # Reconstruct LSTM carry states
        batch_size = obs_batch.shape[0]
        n_lstm_layers = lstm_h_batch.shape[-2] if len(lstm_h_batch.shape) > 1 else 1

        lstm_carry_batch = []
        for i in range(n_lstm_layers):
            h_state = lstm_h_batch[:, i, :] if len(lstm_h_batch.shape) > 2 else lstm_h_batch
            c_state = lstm_c_batch[:, i, :] if len(lstm_c_batch.shape) > 2 else lstm_c_batch
            lstm_carry_batch.append(LSTMState(h=h_state, c=c_state))
        
        # Get current policy outputs
        logits, values, _ = self.network.apply(params, obs_batch, lstm_carry_batch)

        # Clean network outputs
        logits = jnp.where(jnp.isnan(logits), 0.0, logits)
        values = jnp.where(jnp.isnan(values), 0.0, values)
        logits = jnp.clip(logits, -10.0, 10.0)
        values = jnp.clip(values, -100.0, 100.0)
        
        # Clean targets
        returns = jnp.where(jnp.isnan(returns), 0.0, returns)
        returns = jnp.clip(returns, -100.0, 100.0)

        # REINFORCE policy loss (no clipping like PPO)
        action_std = self.config.get('action_std', 1.0)
        action_std = jnp.maximum(action_std, 1e-6)

        action_distribution = distrax.Normal(loc=logits, scale=action_std)
        new_log_probs = action_distribution.log_prob(actions_batch).sum(axis=-1)
        new_log_probs = jnp.where(jnp.isnan(new_log_probs), -10.0, new_log_probs)
        new_log_probs = jnp.clip(new_log_probs, -50.0, 10.0)
        
        # REINFORCE loss: -log_prob * return
        policy_loss = -(new_log_probs * returns).mean()
        
        # Value loss (optional, for baseline)
        value_loss = 0.5 * jnp.square(values - returns).mean()
        
        # Entropy bonus
        entropy = action_distribution.entropy().sum(axis=-1)
        entropy = jnp.where(jnp.isnan(entropy), 0.0, entropy)
        entropy = jnp.clip(entropy, 0.0, 10.0)
        entropy_coeff = self.config.get('entropy_coeff', 0.01)
        entropy_loss = -entropy_coeff * entropy.mean()
        
        # Total loss
        value_coeff = self.config.get('value_coeff', 0.5)
        total_loss = policy_loss + value_coeff * value_loss + entropy_loss
        total_loss = jnp.where(jnp.isnan(total_loss), 0.0, total_loss)
        
        # Compute metrics
        metrics = {
            'total_loss': total_loss,
            'policy_loss': policy_loss,
            'value_loss': value_loss,
            'entropy_loss': entropy_loss,
            'mean_return': returns.mean(),
            'mean_log_prob': new_log_probs.mean()
        }
        
        return total_loss, metrics

    def train_step(self, train_state: train_state.TrainState, episode: EpisodeData,
                   rng_key: chex.PRNGKey) -> Tuple[train_state.TrainState, Dict[str, chex.Array]]:
        """Perform one REINFORCE training step"""

        # Compute returns
        returns = self.compute_returns(episode)
        
        # Flatten episode data
        flat_episode = jax.tree_util.tree_map(
            lambda x: x.reshape(-1, *x.shape[2:]), episode
        )
        flat_returns = returns.reshape(-1)
        
        # Compute gradients and update
        grad_fn = jax.value_and_grad(self.reinforce_loss, has_aux=True)
        (loss, metrics), grads = grad_fn(
            train_state.params, flat_episode, flat_returns, rng_key
        )

        # Clean gradients
        grads = jax.tree_util.tree_map(
            lambda g: jnp.where(jnp.isnan(g), 0.0, g), grads
        )
        grads = jax.tree_util.tree_map(
            lambda g: jnp.where(jnp.isinf(g), jnp.sign(g) * 10.0, g), grads
        )
        
        # Gradient clipping
        grads = jax.tree_util.tree_map(
            lambda g: jnp.clip(g, -10.0, 10.0), grads
        )

        # Apply gradients
        train_state = train_state.apply_gradients(grads=grads)
        
        return train_state, metrics

    def check_early_stopping(self, current_performance: float, update_step: int) -> bool:
        """Check if early stopping criteria are met"""
        self.performance_history.append(current_performance)
        
        if len(self.performance_history) > 50:
            self.performance_history = self.performance_history[-50:]
        
        improvement = current_performance - self.best_performance
        
        if improvement > self.early_stopping_min_delta:
            self.best_performance = current_performance
            self.patience_counter = 0
            logger.info(f"New best performance: {current_performance:.4f} (improvement: {improvement:.4f})")
        else:
            self.patience_counter += 1
            
        if self.patience_counter >= self.early_stopping_patience:
            logger.info(f"Early stopping triggered after {self.patience_counter} steps without improvement")
            logger.info(f"Best performance: {self.best_performance:.4f}, Current: {current_performance:.4f}")
            return True
        
        return False

    def train(self):
        """Main training loop using REINFORCE"""
        logger.info("Starting Plain RL LSTM training with REINFORCE...")

        # Training configuration
        num_updates = self.config.get('num_updates', 1000)
        log_interval = self.config.get('log_interval', 10)
        save_interval = self.config.get('save_interval', 50)

        logger.info(f"Training configuration: {num_updates} updates")
        logger.info(f"Early stopping patience: {self.early_stopping_patience} steps")

        for update in range(num_updates):
            start_time = time.time()

            try:
                # Split RNG for collection and training
                self.rng, collect_rng = random.split(self.rng)
            
                # Collect episode
                episode, self.env_states, self.obs, self.collector_carry = self.collect_episode(
                    self.train_state, self.env_states, self.obs, self.collector_carry, collect_rng
                )

                # Perform REINFORCE training step
                self.rng, train_rng = random.split(self.rng)
                self.train_state, metrics = self.train_step(
                    self.train_state, episode, train_rng
                )
            
                # Logging and early stopping check
                if update % log_interval == 0:
                    elapsed = time.time() - start_time

                    # Compute episode statistics
                    avg_reward = float(episode.rewards.mean())
                    max_return = float(episode.rewards.sum(axis=0).max())
                    total_loss = float(metrics.get('total_loss', 0.0))
                    policy_loss = float(metrics.get('policy_loss', 0.0))
                    value_loss = float(metrics.get('value_loss', 0.0))
                    mean_return = float(metrics.get('mean_return', 0.0))

                    logger.info(
                        f"Update {update}/{num_updates} | "
                        f"Time: {elapsed:.2f}s | "
                        f"Total Loss: {total_loss:.4f} | "
                        f"Policy Loss: {policy_loss:.4f} | "
                        f"Value Loss: {value_loss:.4f} | "
                        f"Avg Reward: {avg_reward:.4f} | "
                        f"Mean Return: {mean_return:.4f} | "
                        f"Max Return: {max_return:.4f} | "
                        f"Patience: {self.patience_counter}/{self.early_stopping_patience}"
                    )

                    # Check early stopping using average reward as performance metric
                    if self.check_early_stopping(avg_reward, update):
                        logger.info(f"Early stopping triggered at update {update}")
                        logger.info(f"Training completed early after {update} updates")
                        break

                    # Log to wandb if enabled
                    if self.config.get('use_wandb', False) and wandb is not None:
                        try:
                            wandb_log = {
                                "losses/total_loss": total_loss,
                                "losses/policy_loss": policy_loss,
                                "losses/value_loss": value_loss,
                                "losses/entropy_loss": float(metrics.get('entropy_loss', 0.0)),
                                "rollout/avg_reward": avg_reward,
                                "rollout/max_reward": float(episode.rewards.max()),
                                "rollout/min_reward": float(episode.rewards.min()),
                                "rollout/avg_episode_return": float(episode.rewards.sum(axis=0).mean()),
                                "rollout/max_episode_return": max_return,
                                "rollout/mean_return": mean_return,
                                "rollout/avg_portfolio_value": float(self.env_states.portfolio_value.mean()),
                                "early_stopping/patience_counter": self.patience_counter,
                                "early_stopping/best_performance": self.best_performance,
                                "global_step": update
                            }
                            wandb.log(wandb_log)
                        except Exception as e:
                            logger.warning(f"Wandb logging failed: {e}")
            
                # Save model periodically
                if update % save_interval == 0 and update > 0:
                    try:
                        self.save_model(f"plain_rl_lstm_update_{update}")
                    except Exception as e:
                        pass

            except Exception as e:
                # Continue training despite errors
                continue

        logger.info("Training complete!")

        # Final cleanup
        if self.config.get('use_wandb', False) and wandb is not None:
            try:
                wandb.finish()
            except Exception as e:
                logger.warning(f"Wandb cleanup failed: {e}")

    def save_model(self, filename: str):
        """Save model parameters"""
        try:
            save_path = Path(self.config.get('model_dir', 'models')) / filename
            save_path = save_path.with_suffix('.pkl')
            save_path.parent.mkdir(parents=True, exist_ok=True)

            # Save model state
            model_state = {
                'params': self.train_state.params,
                'config': self.config,
                'training_step': getattr(self, 'training_step', 0)
            }

            import pickle
            with open(save_path, 'wb') as f:
                pickle.dump(model_state, f)

            logger.info(f"Model saved to {save_path}")

        except Exception as e:
            logger.error(f"Failed to save model: {e}")


def main():
    """Main function with command line argument parsing"""
    parser = argparse.ArgumentParser(
        description="Train Plain RL with LSTM using REINFORCE algorithm and feature combinations",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python train_plain_rl_lstm.py --feature_combination ohlcv+technical
  python train_plain_rl_lstm.py --feature_combination all --num_updates 500
  python train_plain_rl_lstm.py --feature_combination ohlcv+financial+sentiment --use_wandb
        """
    )
    
    # Feature combination argument
    parser.add_argument(
        '--feature_combination', 
        type=str, 
        default='ohlcv+technical',
        help='Feature combination to use (e.g., ohlcv+technical, all, etc.)'
    )
    
    # Training configuration arguments
    parser.add_argument('--num_updates', type=int, default=1000, help='Number of training updates')
    parser.add_argument('--learning_rate', type=float, default=1e-4, help='Learning rate')
    parser.add_argument('--n_envs', type=int, default=8, help='Number of parallel environments')
    parser.add_argument('--episode_length', type=int, default=100, help='Episode length')
    parser.add_argument('--hidden_size', type=int, default=256, help='LSTM hidden size')
    parser.add_argument('--n_lstm_layers', type=int, default=2, help='Number of LSTM layers')
    
    # Data configuration arguments
    parser.add_argument('--data_root', type=str, default='processed_data/', help='Data root directory')
    parser.add_argument('--train_start_date', type=str, default='2024-06-06', help='Training start date')
    parser.add_argument('--train_end_date', type=str, default='2025-03-06', help='Training end date')
    parser.add_argument('--window_size', type=int, default=30, help='Observation window size')
    
    # Early stopping arguments
    parser.add_argument('--early_stopping_patience', type=int, default=200, help='Early stopping patience')
    parser.add_argument('--early_stopping_min_delta', type=float, default=0.001, help='Minimum improvement threshold')
    
    # Other arguments
    parser.add_argument('--use_wandb', action='store_true', help='Use Weights & Biases logging')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    parser.add_argument('--model_dir', type=str, default='models', help='Model save directory')
    parser.add_argument('--list_combinations', action='store_true', help='List available feature combinations')
    
    args = parser.parse_args()
    
    # List available combinations if requested
    if args.list_combinations:
        feature_selector = FeatureSelector()
        feature_selector.print_available_combinations()
        return
    
    # Initialize feature selector
    feature_selector = FeatureSelector()
    
    # Get selected features
    try:
        selected_features = feature_selector.get_features_for_combination(args.feature_combination)
        logger.info(f"Selected {len(selected_features)} features for training")
        
        # Log feature distribution by category
        for category, info in feature_selector.feature_categories.items():
            category_features = [f for f in selected_features if f in info['features']]
            logger.info(f"  {category}: {len(category_features)} features")
            
    except Exception as e:
        logger.error(f"Failed to get features for combination '{args.feature_combination}': {e}")
        feature_selector.print_available_combinations()
        raise
    
    # Create training configuration
    config = {
        # Environment settings
        'seed': args.seed,
        'data_root': args.data_root,
        'stocks': None,  # Will be loaded from stocks.txt
        'train_start_date': args.train_start_date,
        'train_end_date': args.train_end_date,
        'window_size': args.window_size,
        'transaction_cost_rate': 0.005,
        'sharpe_window': 252,
        
        # Data loading settings
        'use_all_features': True,  # Use all available features
        'save_cache': True,
        'cache_format': 'hdf5',
        'force_reload': False,
        'preload_to_gpu': True,
        
        # Training environment
        'n_envs': args.n_envs,
        'episode_length': args.episode_length,
        
        # REINFORCE hyperparameters
        'num_updates': args.num_updates,
        'gamma': 0.99,
        'learning_rate': args.learning_rate,
        'value_coeff': 0.5,
        'entropy_coeff': 0.01,
        'action_std': 0.3,
        
        # Early stopping configuration
        'early_stopping_patience': args.early_stopping_patience,
        'early_stopping_min_delta': args.early_stopping_min_delta,
        
        # Network architecture
        'hidden_size': args.hidden_size,
        'n_lstm_layers': args.n_lstm_layers,
        
        # Logging and monitoring
        'use_wandb': args.use_wandb,
        'log_interval': 20,
        'save_interval': 100,
        'model_dir': args.model_dir,
    }
    
    # Run training
    try:
        logger.info("Creating Plain RL LSTM Trainer...")
        trainer = PlainRLLSTMTrainer(config, selected_features)

        logger.info("Starting training...")
        trainer.train()

        logger.info("Training completed successfully!")
        
        # Save final model
        trainer.save_model(f"final_model_plain_rl_lstm_{args.feature_combination.replace('+', '_')}")
        
    except KeyboardInterrupt:
        logger.info("Training interrupted by user")
        
    except Exception as e:
        logger.error(f"Training failed with error: {e}")
        raise


if __name__ == "__main__":
    main()
