"""
PPO LSTM Training Script with Feature Category Combinations

This script imports the complete PPO LSTM architecture from train_ppo_lstm.py
and allows training with different combinations of feature categories:
- OHLCV (basic price data)
- Technical Indicators (volatility, momentum, moving averages, etc.)
- Financial Indicators (financial statements, ratios)
- Sentiment Indicators (news + social media sentiment)

Usage:
    python train_ppo_feature_combinations.py --feature_combination ohlcv+technical
    python train_ppo_feature_combinations.py --feature_combination all
    python train_ppo_feature_combinations.py --feature_combination ohlcv+financial+sentiment

Author: AI Assistant
Date: 2024
"""

import os
import sys
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any, Set

# Configure JAX to use CPU only (fixes Metal GPU compatibility issues)
os.environ['JAX_PLATFORM_NAME'] = 'cpu'

# Setup logging early
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Fix for JAX 0.6.2 + Flax 0.8.4 compatibility issue
def fix_evaltrace_error():
    """Fix EvalTrace level attribute error in JAX 0.6.2 + Flax 0.8.4"""
    try:
        import flax.core.tracers as tracers
        
        def patched_trace_level(main):
            """Patched version of trace_level that handles missing level attribute"""
            if main:
                if hasattr(main, 'level'):
                    return main.level
                else:
                    return 0
            return float('-inf')
        
        tracers.trace_level = patched_trace_level
        logger.info("Applied monkey patch to fix EvalTrace level attribute error")
    except Exception as e:
        logger.warning(f"Could not apply EvalTrace fix: {e}")

# Apply the fix immediately
fix_evaltrace_error()

import jax
import jax.numpy as jnp
import jax.random as random
import numpy as np
import pandas as pd
from functools import partial

# Import the complete PPO LSTM architecture
from train_ppo_lstm import PPOTrainer, ActorCriticLSTM, LSTMState, Trajectory
from finagent.environment.portfolio_env import JAXVectorizedPortfolioEnv, EnvState
import optax
from flax.training import train_state
import time

# Optional imports for wandb
try:
    import wandb
except ImportError:
    wandb = None

# Logging already setup above

class FeatureSelector:
    """Feature selector for different feature categories"""
    
    def __init__(self):
        # Define feature categories based on actual CSV data and portfolio_env.py analysis
        self.feature_categories = {
            'ohlcv': {
                'description': 'Basic OHLCV price data and simple derived features',
                'features': [
                    # Basic OHLCV (available in CSV)
                    'open', 'high', 'low', 'close', 'volume', 'vwap',
                    
                    # Simple returns (generated by our custom engineer_features)
                    'returns_1d', 'returns_3d', 'returns_5d', 'returns_10d', 'returns_20d',
                    'log_returns_1d', 'log_returns_5d',
                    
                    # Basic price action features (generated by our custom engineer_features)
                    'overnight_gap', 'daily_range', 'close_position',
                    
                    # Lag features (pre-computed in CSV)
                    'open_lag_1', 'open_lag_2', 'open_lag_3', 'open_lag_5', 'open_lag_10', 'open_lag_20',
                    'high_lag_1', 'high_lag_2', 'high_lag_3', 'high_lag_5', 'high_lag_10', 'high_lag_20',
                    'low_lag_1', 'low_lag_2', 'low_lag_3', 'low_lag_5', 'low_lag_10', 'low_lag_20',
                    'close_lag_1', 'close_lag_2', 'close_lag_3', 'close_lag_5', 'close_lag_10', 'close_lag_20',
                    'volume_lag_1', 'volume_lag_2', 'volume_lag_3', 'volume_lag_5', 'volume_lag_10', 'volume_lag_20',
                    
                    # Rolling statistics (pre-computed in CSV)
                    'open_rolling_mean_5', 'open_rolling_mean_20', 'open_rolling_std_20',
                    'high_rolling_mean_5', 'high_rolling_mean_20', 'high_rolling_std_20',
                    'low_rolling_mean_5', 'low_rolling_mean_20', 'low_rolling_std_20',
                    'close_rolling_mean_5', 'close_rolling_mean_20', 'close_rolling_std_20',
                    'close_momentum_5', 'close_momentum_20',
                    'volume_rolling_mean_5', 'volume_rolling_mean_20', 'volume_rolling_std_20'
                ]
            },
            
            'technical': {
                'description': 'Technical indicators and algorithmic trading signals',
                'features': [
                    # Moving averages (pre-computed in CSV)
                    'dma_50', 'dma_200',
                    'dma_50_lag_1', 'dma_50_lag_2', 'dma_50_lag_3', 'dma_50_lag_5', 'dma_50_lag_10', 'dma_50_lag_20',
                    'dma_200_lag_1', 'dma_200_lag_2', 'dma_200_lag_3', 'dma_200_lag_5', 'dma_200_lag_10', 'dma_200_lag_20',
                    'dma_50_rolling_mean_5', 'dma_50_rolling_mean_20', 'dma_50_rolling_std_20',
                    'dma_200_rolling_mean_5', 'dma_200_rolling_mean_20', 'dma_200_rolling_std_20',
                    'dma_cross', 'dma_distance', 'volume_price_trend',
                    
                    # RSI (pre-computed in CSV)
                    'rsi_14',
                    
                    # Volatility features (will be generated by engineer_features)
                    'volatility_5d', 'volatility_10d', 'volatility_20d', 'volatility_30d', 'volatility_60d',
                    'vol_ratio_short_long', 'vol_ratio_5_20',
                    
                    # Momentum features (will be generated)
                    'momentum_5d', 'momentum_10d', 'momentum_20d', 'momentum_60d',
                    'momentum_acceleration_10d',
                    
                    # Moving average features (will be generated)
                    'ma_convergence', 'ma_trend_strength', 'price_position_20d', 
                    'price_above_ma50', 'price_above_ma200',
                    
                    # RSI signals (will be generated)
                    'rsi_oversold', 'rsi_overbought', 'rsi_bullish_divergence', 'rsi_bearish_divergence',
                    'rsi_zscore_20d',
                    
                    # Bollinger Bands (will be generated)
                    'bb_position', 'bb_squeeze', 'bb_breakout_up', 'bb_breakout_down',
                    
                    # Mean reversion signals (will be generated)
                    'price_zscore_20d', 'price_zscore_60d', 'volume_zscore_20d', 'volume_zscore_60d',
                    'price_deviation_50d', 'price_deviation_200d',
                    'mean_reversion_signal_50d', 'mean_reversion_signal_200d',
                    
                    # Breakout signals (will be generated)
                    'price_breakout_20d', 'price_breakdown_20d', 'volume_breakout_20d', 'volume_spike',
                    
                    # Trend following (will be generated)
                    'ma_cross_bullish', 'ma_cross_bearish',
                    
                    # Volatility regime (will be generated)
                    'high_vol_regime', 'low_vol_regime', 'vol_expansion', 'vol_contraction',
                    
                    # Candlestick patterns (will be generated)
                    'body_ratio', 'upper_wick_ratio', 'lower_wick_ratio',
                    'doji_pattern', 'hammer_pattern', 'shooting_star_pattern',
                    
                    # Volume analysis (will be generated)
                    'volume_price_momentum', 'volume_ratio_5d', 'volume_ratio_20d',
                    'volume_trend_10d', 'volume_confirms_price', 'volume_divergence',
                    
                    # Signal aggregation (will be generated)
                    'bullish_signals', 'bearish_signals', 'net_signal_strength',
                    
                    # Risk-adjusted metrics (will be generated)
                    'risk_adjusted_momentum', 'volume_confirmed_trend',
                    
                    # Regime detection (will be generated)
                    'vol_regime_change', 'trend_regime',
                    
                    # Cross-sectional features (will be generated)
                    'momentum_rank_proxy', 'vol_rank_proxy'
                ]
            },
            
            'financial': {
                'description': 'Financial statement metrics and fundamental analysis',
                'features': [
                    # Core Income Statement Features (from CSV)
                    'metric_Revenue', 'metric_TotalRevenue', 'metric_CostofRevenueTotal', 'metric_GrossProfit',
                    'metric_OperatingIncome', 'metric_NetIncomeBeforeTaxes', 'metric_NetIncomeAfterTaxes',
                    'metric_NetIncome', 'metric_DilutedNetIncome', 'metric_DilutedWeightedAverageShares',
                    'metric_DilutedEPSExcludingExtraOrdItems', 'metric_DPS-CommonStockPrimaryIssue',
                    
                    # Core Balance Sheet Features (from CSV)
                    'metric_Cash', 'metric_ShortTermInvestments', 'metric_CashandShortTermInvestments',
                    'metric_TotalCurrentAssets', 'metric_TotalAssets', 'metric_TotalCurrentLiabilities',
                    'metric_TotalLiabilities', 'metric_TotalEquity', 'metric_TotalCommonSharesOutstanding',
                    
                    # Core Cash Flow Features (from CSV)
                    'metric_CashfromOperatingActivities', 'metric_CapitalExpenditures', 
                    'metric_CashfromInvestingActivities', 'metric_CashfromFinancingActivities',
                    'metric_NetChangeinCash', 'metric_TotalCashDividendsPaid',
                    
                    # Key Financial Metrics (from CSV)
                    'metric_freeCashFlowtrailing12Month', 'metric_freeCashFlowMostRecentFiscalYear',
                    'metric_periodLength', 'metric_periodType',
                    
                    # Additional financial metrics (from CSV)
                    'metric_pPerEExcludingExtraordinaryItemsMostRecentFiscalYear',
                    'metric_currentDividendYieldCommonStockPrimaryIssueLTM',
                    'metric_priceToBookMostRecentFiscalYear',
                    'metric_priceToFreeCashFlowPerShareTrailing12Months',
                    'metric_pPerEBasicExcludingExtraordinaryItemsTTM',
                    'metric_pPerEIncludingExtraordinaryItemsTTM',
                    'metric_returnOnAverageEquityMostRecentFiscalYear',
                    'metric_returnOnInvestmentMostRecentFiscalYear',
                    'metric_netProfitMarginPercentTrailing12Month',
                    'metric_operatingMarginTrailing12Month',
                    'metric_grossMarginTrailing12Month',
                    'metric_currentRatioMostRecentFiscalYear',
                    'metric_quickRatioMostRecentFiscalYear',
                    'metric_totalDebtPerTotalEquityMostRecentFiscalYear',
                    'metric_netInterestCoverageMostRecentFiscalYear',
                    'metric_marketCap',
                    'metric_beta'
                ]
            },
            
            'sentiment': {
                'description': 'News and social media sentiment indicators',
                'features': [
                    # Reddit sentiment features (from CSV)
                    'reddit_title_sentiments_mean', 'reddit_title_sentiments_std',
                    'reddit_body_sentiments', 'reddit_body_sentiments_std',
                    'reddit_score_mean', 'reddit_score_sum', 'reddit_posts_count', 'reddit_comments_sum',
                    
                    # News sentiment features (from CSV)
                    'news_sentiment_mean', 'news_articles_count', 'news_sentiment_std', 'news_sources',
                    
                    # Sentiment features that will be generated by engineer_features
                    'sentiment_momentum_3d', 'sentiment_momentum_5d',
                    'sentiment_extreme_positive', 'sentiment_extreme_negative'
                ]
            }
        }
    
    def get_features_for_combination(self, combination: str) -> List[str]:
        """
        Get list of features for a given combination string.
        
        Args:
            combination: Feature combination string like 'ohlcv+technical' or 'all'
            
        Returns:
            List of feature names
        """
        if combination.lower() == 'all':
            # Return all features from all categories
            all_features = []
            for category_features in self.feature_categories.values():
                all_features.extend(category_features['features'])
            return list(set(all_features))  # Remove duplicates
        
        # Parse combination string
        categories = [cat.strip().lower() for cat in combination.split('+')]
        
        # Validate categories
        valid_categories = set(self.feature_categories.keys())
        invalid_categories = set(categories) - valid_categories
        if invalid_categories:
            raise ValueError(f"Invalid feature categories: {invalid_categories}. "
                           f"Valid categories are: {list(valid_categories)}")
        
        # Combine features from selected categories
        selected_features = []
        for category in categories:
            selected_features.extend(self.feature_categories[category]['features'])
        
        # Remove duplicates and ensure 'close' is always first
        selected_features = list(set(selected_features))
        if 'close' in selected_features:
            selected_features.remove('close')
            selected_features = ['close'] + selected_features
        
        return selected_features
    
    def print_available_combinations(self):
        """Print available feature combinations"""
        print("\n=== Available Feature Categories ===")
        for category, info in self.feature_categories.items():
            print(f"\n{category.upper()}: {info['description']}")
            print(f"  Features ({len(info['features'])}): {', '.join(info['features'][:5])}...")
        
        print(f"\n=== Example Combinations ===")
        print("• ohlcv - Basic price data only")
        print("• technical - Technical indicators only")
        print("• financial - Financial metrics only")
        print("• sentiment - Sentiment data only")
        print("• ohlcv+technical - Price data + technical indicators")
        print("• ohlcv+financial - Price data + financial metrics")
        print("• ohlcv+sentiment - Price data + sentiment")
        print("• technical+financial - Technical + financial indicators")
        print("• technical+sentiment - Technical + sentiment indicators")
        print("• financial+sentiment - Financial + sentiment indicators")
        print("• ohlcv+technical+financial - Price + technical + financial")
        print("• ohlcv+technical+sentiment - Price + technical + sentiment")
        print("• ohlcv+financial+sentiment - Price + financial + sentiment")
        print("• technical+financial+sentiment - All except basic price data")
        print("• all - All available features")


class CustomPortfolioEnv(JAXVectorizedPortfolioEnv):
    """Custom portfolio environment with feature selection capability"""
    
    def __init__(self, selected_features: List[str], **kwargs):
        """
        Initialize environment with selected features only.
        
        Args:
            selected_features: List of features to use for training
            **kwargs: Other environment arguments
        """
        # Store selected features
        self.selected_features = selected_features
        logger.info(f"Initializing environment with {len(selected_features)} selected features")
        logger.info(f"Selected features: {selected_features[:10]}..." if len(selected_features) > 10 else f"Selected features: {selected_features}")
        
        # Initialize parent class
        super().__init__(**kwargs)
        
        # Override features in data loader
        self.data_loader.features = selected_features
        
        # Recalculate observation dimension based on selected features
        self._recalculate_observation_dim()
    
    def _recalculate_observation_dim(self):
        """Recalculate observation dimension based on selected features"""
        self.n_features = len(self.selected_features)
        
        # Updated observation size calculation
        obs_size = (
            (self.window_size * self.n_stocks * self.n_features) +  # Historical features
            self.n_stocks * 2 +                                     # Current open prices + gaps
            self.action_dim +                                       # Portfolio weights
            self.n_stocks +                                         # Short position flags
            8                                                       # Market state (8 elements)
        )
        
        self.obs_dim = obs_size
        
        logger.info(f"Recalculated observation dimension: {self.obs_dim}")
        logger.info(f"Features per stock: {self.n_features}")
        logger.info(f"Historical window features: {self.window_size * self.n_stocks * self.n_features}")
    
    def engineer_features(self, df: pd.DataFrame, stock: str) -> pd.DataFrame:
        """
        Override feature engineering to only generate selected features.
        This avoids generating hundreds of unnecessary features.
        """
        # Start with basic OHLCV columns
        df_engineered = df.copy()
        
        # Only generate features that are in our selected_features list
        # This prevents generating hundreds of unnecessary features
        
        # Basic returns (if requested)
        if 'returns_1d' in self.selected_features:
            df_engineered['returns_1d'] = df['close'].pct_change()
        if 'returns_3d' in self.selected_features:
            df_engineered['returns_3d'] = df['close'].pct_change(periods=3)
        if 'returns_5d' in self.selected_features:
            df_engineered['returns_5d'] = df['close'].pct_change(periods=5)
        if 'returns_10d' in self.selected_features:
            df_engineered['returns_10d'] = df['close'].pct_change(periods=10)
        if 'returns_20d' in self.selected_features:
            df_engineered['returns_20d'] = df['close'].pct_change(periods=20)
        
        # Log returns (if requested)
        if 'log_returns_1d' in self.selected_features:
            df_engineered['log_returns_1d'] = np.log(df['close'] / df['close'].shift(1))
        if 'log_returns_5d' in self.selected_features:
            df_engineered['log_returns_5d'] = np.log(df['close'] / df['close'].shift(5))
        
        # Price action features (if requested)
        if 'overnight_gap' in self.selected_features and all(col in df.columns for col in ['open', 'close']):
            df_engineered['overnight_gap'] = df['open'] / df['close'].shift(1) - 1.0
        if 'daily_range' in self.selected_features and all(col in df.columns for col in ['high', 'low', 'open']):
            df_engineered['daily_range'] = (df['high'] - df['low']) / df['open']
        if 'close_position' in self.selected_features and all(col in df.columns for col in ['high', 'low', 'close']):
            df_engineered['close_position'] = (df['close'] - df['low']) / (df['high'] - df['low'] + 1e-8)
        
        # Now filter to only include features that are in our selected_features list
        # and that actually exist in the dataframe
        available_features = [f for f in self.selected_features if f in df_engineered.columns]
        
        # Create filtered dataframe with only selected features
        filtered_df = df_engineered[available_features].copy()
        
        # Ensure 'close' is always first
        if 'close' in filtered_df.columns:
            cols = ['close'] + [c for c in filtered_df.columns if c != 'close']
            filtered_df = filtered_df[cols]
        
        logger.info(f"Generated {len(filtered_df.columns)} features for {stock}: {list(filtered_df.columns)}")
        
        return filtered_df.fillna(0.0)

class CurriculumConfig:
    def __init__(self):
        # Define curriculum stages here
        self.stages = {
            1: {"stage_num": 1, "name": "Exploration", "description": "Initial exploration stage"},
            2: {"stage_num": 2, "name": "Refinement", "description": "Refinement of learned policies"},
            3: {"stage_num": 3, "name": "Optimization", "description": "Final optimization stage"}
        }

    def get_stage(self, stage_num):
        return self.stages.get(stage_num, None)

class FeatureCombinationPPOTrainer(PPOTrainer):
    """PPO Trainer with feature combination support, curriculum learning, and robust early stopping."""

    def __init__(self, config: Dict[str, Any], selected_features: List[str]):
        """
        Initialize trainer with selected features and curriculum learning support.
        
        Args:
            config: Training configuration
            selected_features: List of features to use
        """
        self.selected_features = selected_features
        self.config = config
        self.curriculum_stage = config.get('curriculum_stage', None)
        self.curriculum_config = config.get('curriculum_config', None)
        
        # Apply curriculum stage settings if provided
        if self.curriculum_stage and self.curriculum_config:
            self._apply_curriculum_settings()
        
        # Initialize parent class
        super().__init__(self.config, selected_features)
        
        # Initialize training state and metrics
        self._initialize_training_state()

    def _apply_curriculum_settings(self):
        """Apply curriculum-specific settings to the configuration."""
        stage = self.curriculum_config.get_stage(self.curriculum_stage)
        logger.info(f"Initializing Stage {stage.stage_num}: {stage.name}")
        logger.info(f"Description: {stage.description}")
        
        # Update config with stage-specific hyperparameters
        self.config.update({
            'action_std': stage.exploration_std,
            'entropy_coeff': stage.entropy_coeff,
            'clip_eps': stage.clip_eps,
            'learning_rate': stage.learning_rate,
            'num_updates': stage.num_epochs,
            'final_action_std': stage.exploration_std * 0.5,
            'action_std_decay_steps': stage.num_epochs // 2,
            'final_entropy_coeff': stage.entropy_coeff * 0.5,
            'entropy_decay_steps': int(stage.num_epochs * 0.8),
        })
        
        # Store stage-specific parameters
        self.epsilon_uniform = stage.epsilon_uniform
        self.reward_scaling = stage.reward_scaling
        self.enable_constraints = stage.enable_constraints
        
        # Early stopping settings
        if stage.stage_num == 1:
            self.config.update({'early_stopping_patience': 30, 'early_stopping_min_delta': 0.001})
        elif stage.stage_num == 2:
            self.config.update({'early_stopping_patience': 50, 'early_stopping_min_delta': 0.005})
        else:
            self.config.update({'early_stopping_patience': 75, 'early_stopping_min_delta': 0.01})

    def _initialize_training_state(self):
        """Initialize training state and metrics."""
        self.early_training_metrics = {
            'portfolio_values': [],
            'rewards': [],
            'variances': [],
            'losses': []
        }
        self.best_performance = -float('inf')
        self.patience_counter = 0
        self.should_stop_early = False

    def train(self):
        """Train the PPO model with curriculum learning and early stopping."""
        logger.info("Starting PPO training...")
        num_updates = self.config.get('num_updates', 1000)
        log_interval = self.config.get('log_interval', 10)
        save_interval = self.config.get('save_interval', 50)

        for update in range(num_updates):
            start_time = time.time()

            try:
                # Collect trajectory
                trajectory = self._collect_trajectory(update)

                # Normalize rewards
                trajectory = trajectory._replace(rewards=self.normalize_rewards(trajectory.rewards))

                # Update hyperparameters
                self._update_hyperparameters(update)

                # Perform training step
                self._train_step(trajectory, update)

                # Log progress
                if update % log_interval == 0:
                    self._log_progress(trajectory, update, start_time)

                # Check early stopping
                if self.check_early_stopping(trajectory, update):
                    logger.info("Early stopping triggered. Training complete.")
                    break

                # Save model periodically
                if update % save_interval == 0 and update > 0:
                    self.save_model(f"model_checkpoint_{update}")

            except Exception as e:
                logger.error(f"Error during training step {update}: {e}")
                continue

        logger.info("Training complete!")

    def _collect_trajectory(self, update: int) -> Trajectory:
        """Collect trajectory from the environment."""
        self.rng, collect_rng = random.split(self.rng)
        trajectory, self.env_states, self.obs, self.collector_carry = self.collect_trajectory(
            self.train_state, self.env_states, self.obs, self.collector_carry, collect_rng
        )
        return trajectory

    def _update_hyperparameters(self, update: int):
        """Update hyperparameters dynamically during training."""
        self.config['action_std'] = self.get_current_action_std(update)
        self.config['entropy_coeff'] = self.get_current_entropy_coeff(update)

    def _train_step(self, trajectory: Trajectory, update: int):
        """Perform a single training step."""
        self.rng, train_rng = random.split(self.rng)
        self.train_state, metrics = self.train_step(
            self.train_state, trajectory, self._get_last_values(), train_rng
        )
        self._track_metrics(metrics)

    def _get_last_values(self) -> jnp.ndarray:
        """Get bootstrap values for GAE."""
        try:
            _, last_values, _ = self.train_state.apply_fn(
                self.train_state.params, self.obs, self.collector_carry
            )
            return jnp.where(jnp.isnan(last_values), 0.0, last_values)
        except Exception:
            return jnp.zeros(self.config.get('n_envs', 8))

    def _track_metrics(self, metrics: Dict[str, Any]):
        """Track training metrics."""
        if 'approx_kl' in metrics:
            self.kl_divergence_history.append(float(metrics['approx_kl']))
            if len(self.kl_divergence_history) > 50:
                self.kl_divergence_history.pop(0)

    def _log_progress(self, trajectory: Trajectory, update: int, start_time: float):
        """Log training progress."""
        elapsed = time.time() - start_time
        avg_reward = float(trajectory.rewards.mean())
        logger.info(
            f"Update {update} | Time: {elapsed:.2f}s | Avg Reward: {avg_reward:.4f}"
        )

    def check_early_stopping(self, trajectory: Trajectory, update: int) -> bool:
        """Check if early stopping criteria are met."""
        metrics = self.compute_robust_metrics(trajectory)
        sharpe_improvement = metrics['sharpe_ratio'] - self.best_performance

        if sharpe_improvement > self.config['early_stopping_min_delta']:
            self.best_performance = metrics['sharpe_ratio']
            self.patience_counter = 0
        else:
            self.patience_counter += 1

        if self.patience_counter >= self.config['early_stopping_patience']:
            return True

        return False
    


def run_training_with_combination(feature_combination: str, config: Dict[str, Any]):
    """
    Run training with a specific feature combination.
    
    Args:
        feature_combination: Feature combination string
        config: Training configuration
    """
    logger.info(f"Starting training with feature combination: {feature_combination}")
    
    # Initialize feature selector
    feature_selector = FeatureSelector()
    
    # Get selected features
    try:
        selected_features = feature_selector.get_features_for_combination(feature_combination)
        logger.info(f"Selected {len(selected_features)} features for training")
        
        # Log feature distribution by category
        for category, info in feature_selector.feature_categories.items():
            category_features = [f for f in selected_features if f in info['features']]
            logger.info(f"  {category}: {len(category_features)} features")
            
    except Exception as e:
        logger.error(f"Failed to get features for combination '{feature_combination}': {e}")
        feature_selector.print_available_combinations()
        raise
    
    # Update config with feature combination info
    config['feature_combination'] = feature_combination
    config['selected_features'] = selected_features
    config['model_name'] = f"ppo_lstm_{feature_combination.replace('+', '_')}"
    
    # Initialize trainer
    try:
        trainer = FeatureCombinationPPOTrainer(config, selected_features)
        logger.info("Feature combination PPO trainer initialized successfully")
        
    except Exception as e:
        logger.error(f"Failed to initialize trainer: {e}")
        raise
    
    # Run training
    try:
        logger.info("Starting training...")
        trainer.train()
        logger.info("Training completed successfully!")
        
        # Save final model
        trainer.save_model(f"final_model_{feature_combination.replace('+', '_')}")
        
    except Exception as e:
        logger.error(f"Training failed: {e}")
        raise


def main():
    """Main function with command line argument parsing"""
    parser = argparse.ArgumentParser(
        description="Train PPO LSTM with different feature combinations",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python train_ppo_feature_combinations.py --feature_combination ohlcv+technical
  python train_ppo_feature_combinations.py --feature_combination all --num_updates 500
  python train_ppo_feature_combinations.py --feature_combination ohlcv+financial+sentiment --use_wandb
  python train_ppo_feature_combinations.py --feature_combination ohlcv+technical --curriculum_stage 1
  python train_ppo_feature_combinations.py --feature_combination ohlcv+technical --auto_curriculum
        """
    )
    
    # Feature combination argument
    parser.add_argument(
        '--feature_combination', 
        type=str, 
        default='ohlcv+technical',
        help='Feature combination to use (e.g., ohlcv+technical, all, etc.)'
    )
    
    # Curriculum learning arguments
    parser.add_argument('--curriculum_stage', type=int, choices=[1, 2, 3],
                        help='Specific curriculum stage to run (1=exploration, 2=refinement, 3=optimization)')
    parser.add_argument('--auto_curriculum', action='store_true',
                        help='Automatically progress through all curriculum stages')
    parser.add_argument('--start_stage', type=int, default=1, choices=[1, 2, 3],
                        help='Stage to start from in auto mode')
    
    # Training configuration arguments
    parser.add_argument('--num_updates', type=int, default=1000, help='Number of training updates')
    parser.add_argument('--learning_rate', type=float, default=3e-4, help='Learning rate')
    parser.add_argument('--n_envs', type=int, default=8, help='Number of parallel environments')
    parser.add_argument('--n_steps', type=int, default=32, help='Number of steps per environment')
    parser.add_argument('--ppo_epochs', type=int, default=4, help='PPO epochs per update')
    parser.add_argument('--ppo_batch_size', type=int, default=128, help='PPO batch size')
    parser.add_argument('--hidden_size', type=int, default=256, help='LSTM hidden size')
    parser.add_argument('--n_lstm_layers', type=int, default=2, help='Number of LSTM layers')
    
    # Data configuration arguments
    parser.add_argument('--data_root', type=str, default='processed_data/', help='Data root directory')
    parser.add_argument('--train_start_date', type=str, default='2024-06-06', help='Training start date')
    parser.add_argument('--train_end_date', type=str, default='2025-03-06', help='Training end date')
    parser.add_argument('--window_size', type=int, default=30, help='Observation window size')
    
    # Early stopping arguments
    parser.add_argument('--early_stopping_patience', type=int, default=150, help='Early stopping patience (steps without improvement)')
    parser.add_argument('--early_stopping_min_delta', type=float, default=0.005, help='Minimum improvement threshold for early stopping')
    
    # Other arguments
    parser.add_argument('--use_wandb', action='store_true', help='Use Weights & Biases logging')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    parser.add_argument('--model_dir', type=str, default='models', help='Model save directory')
    parser.add_argument('--list_combinations', action='store_true', help='List available feature combinations')
    
    args = parser.parse_args()
    
    # List available combinations if requested
    if args.list_combinations:
        feature_selector = FeatureSelector()
        feature_selector.print_available_combinations()
        return
    
    # Create training configuration
    config = {
        # Environment settings
        'seed': args.seed,
        'data_root': args.data_root,
        'train_start_date': args.train_start_date,
        'train_end_date': args.train_end_date,
        'window_size': args.window_size,
        'n_envs': args.n_envs,
        'n_steps': args.n_steps,
        'num_updates': args.num_updates,
        'learning_rate': args.learning_rate,
        'ppo_epochs': args.ppo_epochs,
        'ppo_batch_size': args.ppo_batch_size,
        'hidden_size': args.hidden_size,
        'n_lstm_layers': args.n_lstm_layers,
        'use_wandb': args.use_wandb,
        'early_stopping_patience': args.early_stopping_patience,
        'early_stopping_min_delta': args.early_stopping_min_delta,
        'model_dir': args.model_dir,
    }
    
    # Curriculum learning configuration
    if args.curriculum_stage or args.auto_curriculum:
        curriculum_config = CurriculumConfig()
        config['curriculum_config'] = curriculum_config
        config['curriculum_stage'] = args.curriculum_stage
    
    # Run training
    try:
        if args.auto_curriculum:
            logger.info("Running FULL CURRICULUM (all stages)")
            for stage_num in range(args.start_stage, 4):
                config['curriculum_stage'] = stage_num
                run_training_with_combination(args.feature_combination, config)
        else:
            run_training_with_combination(args.feature_combination, config)
        
    except KeyboardInterrupt:
        logger.info("Training interrupted by user")
        
    except Exception as e:
        logger.error(f"Training failed with error: {e}")
        raise


if __name__ == "__main__":
    main()