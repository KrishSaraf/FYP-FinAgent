"""
PPO LSTM Training Script with Feature Category Combinations

This script imports the complete PPO LSTM architecture from train_ppo_lstm.py
and allows training with different combinations of feature categories:
- OHLCV (basic price data)
- Technical Indicators (volatility, momentum, moving averages, etc.)
- Financial Indicators (financial statements, ratios)
- Sentiment Indicators (news + social media sentiment)

Usage:
    python train_ppo_feature_combinations.py --feature_combination ohlcv+technical
    python train_ppo_feature_combinations.py --feature_combination all
    python train_ppo_feature_combinations.py --feature_combination ohlcv+financial+sentiment

Author: AI Assistant
Date: 2024
"""

import os
import sys
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any, Set

# Configure JAX to use CPU only (fixes Metal GPU compatibility issues)
os.environ['JAX_PLATFORM_NAME'] = 'cpu'

# Setup logging early
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Fix for JAX 0.6.2 + Flax 0.8.4 compatibility issue
def fix_evaltrace_error():
    """Fix EvalTrace level attribute error in JAX 0.6.2 + Flax 0.8.4"""
    try:
        import flax.core.tracers as tracers
        
        def patched_trace_level(main):
            """Patched version of trace_level that handles missing level attribute"""
            if main:
                if hasattr(main, 'level'):
                    return main.level
                else:
                    return 0
            return float('-inf')
        
        tracers.trace_level = patched_trace_level
        logger.info("Applied monkey patch to fix EvalTrace level attribute error")
    except Exception as e:
        logger.warning(f"Could not apply EvalTrace fix: {e}")

# Apply the fix immediately
fix_evaltrace_error()

import jax
import jax.numpy as jnp
import jax.random as random
import numpy as np
import pandas as pd
from functools import partial

# Import the complete PPO LSTM architecture
from train_ppo_lstm import PPOTrainer, ActorCriticLSTM, LSTMState, Trajectory
from finagent.environment.portfolio_env import JAXVectorizedPortfolioEnv, EnvState
import optax
from flax.training import train_state
import time

# Optional imports for wandb
try:
    import wandb
except ImportError:
    wandb = None

# Logging already setup above

class FeatureSelector:
    """Feature selector for different feature categories"""
    
    def __init__(self):
        # Define feature categories based on actual CSV data and portfolio_env.py analysis
        self.feature_categories = {
            'ohlcv': {
                'description': 'Basic OHLCV price data and simple derived features',
                'features': [
                    # Basic OHLCV (available in CSV)
                    'open', 'high', 'low', 'close', 'volume', 'vwap',
                    
                    # Simple returns (generated by our custom engineer_features)
                    'returns_1d', 'returns_3d', 'returns_5d', 'returns_10d', 'returns_20d',
                    'log_returns_1d', 'log_returns_5d',
                    
                    # Basic price action features (generated by our custom engineer_features)
                    'overnight_gap', 'daily_range', 'close_position',
                    
                    # Lag features (pre-computed in CSV)
                    'open_lag_1', 'open_lag_2', 'open_lag_3', 'open_lag_5', 'open_lag_10', 'open_lag_20',
                    'high_lag_1', 'high_lag_2', 'high_lag_3', 'high_lag_5', 'high_lag_10', 'high_lag_20',
                    'low_lag_1', 'low_lag_2', 'low_lag_3', 'low_lag_5', 'low_lag_10', 'low_lag_20',
                    'close_lag_1', 'close_lag_2', 'close_lag_3', 'close_lag_5', 'close_lag_10', 'close_lag_20',
                    'volume_lag_1', 'volume_lag_2', 'volume_lag_3', 'volume_lag_5', 'volume_lag_10', 'volume_lag_20',
                    
                    # Rolling statistics (pre-computed in CSV)
                    'open_rolling_mean_5', 'open_rolling_mean_20', 'open_rolling_std_20',
                    'high_rolling_mean_5', 'high_rolling_mean_20', 'high_rolling_std_20',
                    'low_rolling_mean_5', 'low_rolling_mean_20', 'low_rolling_std_20',
                    'close_rolling_mean_5', 'close_rolling_mean_20', 'close_rolling_std_20',
                    'close_momentum_5', 'close_momentum_20',
                    'volume_rolling_mean_5', 'volume_rolling_mean_20', 'volume_rolling_std_20'
                ]
            },
            
            'technical': {
                'description': 'Technical indicators and algorithmic trading signals',
                'features': [
                    # Moving averages (pre-computed in CSV)
                    'dma_50', 'dma_200',
                    'dma_50_lag_1', 'dma_50_lag_2', 'dma_50_lag_3', 'dma_50_lag_5', 'dma_50_lag_10', 'dma_50_lag_20',
                    'dma_200_lag_1', 'dma_200_lag_2', 'dma_200_lag_3', 'dma_200_lag_5', 'dma_200_lag_10', 'dma_200_lag_20',
                    'dma_50_rolling_mean_5', 'dma_50_rolling_mean_20', 'dma_50_rolling_std_20',
                    'dma_200_rolling_mean_5', 'dma_200_rolling_mean_20', 'dma_200_rolling_std_20',
                    'dma_cross', 'dma_distance', 'volume_price_trend',
                    
                    # RSI (pre-computed in CSV)
                    'rsi_14',
                    
                    # Volatility features (will be generated by engineer_features)
                    'volatility_5d', 'volatility_10d', 'volatility_20d', 'volatility_30d', 'volatility_60d',
                    'vol_ratio_short_long', 'vol_ratio_5_20',
                    
                    # Momentum features (will be generated)
                    'momentum_5d', 'momentum_10d', 'momentum_20d', 'momentum_60d',
                    'momentum_acceleration_10d',
                    
                    # Moving average features (will be generated)
                    'ma_convergence', 'ma_trend_strength', 'price_position_20d', 
                    'price_above_ma50', 'price_above_ma200',
                    
                    # RSI signals (will be generated)
                    'rsi_oversold', 'rsi_overbought', 'rsi_bullish_divergence', 'rsi_bearish_divergence',
                    'rsi_zscore_20d',
                    
                    # Bollinger Bands (will be generated)
                    'bb_position', 'bb_squeeze', 'bb_breakout_up', 'bb_breakout_down',
                    
                    # Mean reversion signals (will be generated)
                    'price_zscore_20d', 'price_zscore_60d', 'volume_zscore_20d', 'volume_zscore_60d',
                    'price_deviation_50d', 'price_deviation_200d',
                    'mean_reversion_signal_50d', 'mean_reversion_signal_200d',
                    
                    # Breakout signals (will be generated)
                    'price_breakout_20d', 'price_breakdown_20d', 'volume_breakout_20d', 'volume_spike',
                    
                    # Trend following (will be generated)
                    'ma_cross_bullish', 'ma_cross_bearish',
                    
                    # Volatility regime (will be generated)
                    'high_vol_regime', 'low_vol_regime', 'vol_expansion', 'vol_contraction',
                    
                    # Candlestick patterns (will be generated)
                    'body_ratio', 'upper_wick_ratio', 'lower_wick_ratio',
                    'doji_pattern', 'hammer_pattern', 'shooting_star_pattern',
                    
                    # Volume analysis (will be generated)
                    'volume_price_momentum', 'volume_ratio_5d', 'volume_ratio_20d',
                    'volume_trend_10d', 'volume_confirms_price', 'volume_divergence',
                    
                    # Signal aggregation (will be generated)
                    'bullish_signals', 'bearish_signals', 'net_signal_strength',
                    
                    # Risk-adjusted metrics (will be generated)
                    'risk_adjusted_momentum', 'volume_confirmed_trend',
                    
                    # Regime detection (will be generated)
                    'vol_regime_change', 'trend_regime',
                    
                    # Cross-sectional features (will be generated)
                    'momentum_rank_proxy', 'vol_rank_proxy'
                ]
            },
            
            'financial': {
                'description': 'Financial statement metrics and fundamental analysis',
                'features': [
                    # Core Income Statement Features (from CSV)
                    'metric_Revenue', 'metric_TotalRevenue', 'metric_CostofRevenueTotal', 'metric_GrossProfit',
                    'metric_OperatingIncome', 'metric_NetIncomeBeforeTaxes', 'metric_NetIncomeAfterTaxes',
                    'metric_NetIncome', 'metric_DilutedNetIncome', 'metric_DilutedWeightedAverageShares',
                    'metric_DilutedEPSExcludingExtraOrdItems', 'metric_DPS-CommonStockPrimaryIssue',
                    
                    # Core Balance Sheet Features (from CSV)
                    'metric_Cash', 'metric_ShortTermInvestments', 'metric_CashandShortTermInvestments',
                    'metric_TotalCurrentAssets', 'metric_TotalAssets', 'metric_TotalCurrentLiabilities',
                    'metric_TotalLiabilities', 'metric_TotalEquity', 'metric_TotalCommonSharesOutstanding',
                    
                    # Core Cash Flow Features (from CSV)
                    'metric_CashfromOperatingActivities', 'metric_CapitalExpenditures', 
                    'metric_CashfromInvestingActivities', 'metric_CashfromFinancingActivities',
                    'metric_NetChangeinCash', 'metric_TotalCashDividendsPaid',
                    
                    # Key Financial Metrics (from CSV)
                    'metric_freeCashFlowtrailing12Month', 'metric_freeCashFlowMostRecentFiscalYear',
                    'metric_periodLength', 'metric_periodType',
                    
                    # Additional financial metrics (from CSV)
                    'metric_pPerEExcludingExtraordinaryItemsMostRecentFiscalYear',
                    'metric_currentDividendYieldCommonStockPrimaryIssueLTM',
                    'metric_priceToBookMostRecentFiscalYear',
                    'metric_priceToFreeCashFlowPerShareTrailing12Months',
                    'metric_pPerEBasicExcludingExtraordinaryItemsTTM',
                    'metric_pPerEIncludingExtraordinaryItemsTTM',
                    'metric_returnOnAverageEquityMostRecentFiscalYear',
                    'metric_returnOnInvestmentMostRecentFiscalYear',
                    'metric_netProfitMarginPercentTrailing12Month',
                    'metric_operatingMarginTrailing12Month',
                    'metric_grossMarginTrailing12Month',
                    'metric_currentRatioMostRecentFiscalYear',
                    'metric_quickRatioMostRecentFiscalYear',
                    'metric_totalDebtPerTotalEquityMostRecentFiscalYear',
                    'metric_netInterestCoverageMostRecentFiscalYear',
                    'metric_marketCap',
                    'metric_beta'
                ]
            },
            
            'sentiment': {
                'description': 'News and social media sentiment indicators',
                'features': [
                    # Reddit sentiment features (from CSV)
                    'reddit_title_sentiments_mean', 'reddit_title_sentiments_std',
                    'reddit_body_sentiments', 'reddit_body_sentiments_std',
                    'reddit_score_mean', 'reddit_score_sum', 'reddit_posts_count', 'reddit_comments_sum',
                    
                    # News sentiment features (from CSV)
                    'news_sentiment_mean', 'news_articles_count', 'news_sentiment_std', 'news_sources',
                    
                    # Sentiment features that will be generated by engineer_features
                    'sentiment_momentum_3d', 'sentiment_momentum_5d',
                    'sentiment_extreme_positive', 'sentiment_extreme_negative'
                ]
            }
        }
    
    def get_features_for_combination(self, combination: str) -> List[str]:
        """
        Get list of features for a given combination string.
        
        Args:
            combination: Feature combination string like 'ohlcv+technical' or 'all'
            
        Returns:
            List of feature names
        """
        if combination.lower() == 'all':
            # Return all features from all categories
            all_features = []
            for category_features in self.feature_categories.values():
                all_features.extend(category_features['features'])
            return list(set(all_features))  # Remove duplicates
        
        # Parse combination string
        categories = [cat.strip().lower() for cat in combination.split('+')]
        
        # Validate categories
        valid_categories = set(self.feature_categories.keys())
        invalid_categories = set(categories) - valid_categories
        if invalid_categories:
            raise ValueError(f"Invalid feature categories: {invalid_categories}. "
                           f"Valid categories are: {list(valid_categories)}")
        
        # Combine features from selected categories
        selected_features = []
        for category in categories:
            selected_features.extend(self.feature_categories[category]['features'])
        
        # Remove duplicates and ensure 'close' is always first
        selected_features = list(set(selected_features))
        if 'close' in selected_features:
            selected_features.remove('close')
            selected_features = ['close'] + selected_features
        
        return selected_features
    
    def print_available_combinations(self):
        """Print available feature combinations"""
        print("\n=== Available Feature Categories ===")
        for category, info in self.feature_categories.items():
            print(f"\n{category.upper()}: {info['description']}")
            print(f"  Features ({len(info['features'])}): {', '.join(info['features'][:5])}...")
        
        print(f"\n=== Example Combinations ===")
        print("• ohlcv - Basic price data only")
        print("• technical - Technical indicators only")
        print("• financial - Financial metrics only")
        print("• sentiment - Sentiment data only")
        print("• ohlcv+technical - Price data + technical indicators")
        print("• ohlcv+financial - Price data + financial metrics")
        print("• ohlcv+sentiment - Price data + sentiment")
        print("• technical+financial - Technical + financial indicators")
        print("• technical+sentiment - Technical + sentiment indicators")
        print("• financial+sentiment - Financial + sentiment indicators")
        print("• ohlcv+technical+financial - Price + technical + financial")
        print("• ohlcv+technical+sentiment - Price + technical + sentiment")
        print("• ohlcv+financial+sentiment - Price + financial + sentiment")
        print("• technical+financial+sentiment - All except basic price data")
        print("• all - All available features")


class CustomPortfolioEnv(JAXVectorizedPortfolioEnv):
    """Custom portfolio environment with feature selection capability"""
    
    def __init__(self, selected_features: List[str], **kwargs):
        """
        Initialize environment with selected features only.
        
        Args:
            selected_features: List of features to use for training
            **kwargs: Other environment arguments
        """
        # Store selected features
        self.selected_features = selected_features
        logger.info(f"Initializing environment with {len(selected_features)} selected features")
        logger.info(f"Selected features: {selected_features[:10]}..." if len(selected_features) > 10 else f"Selected features: {selected_features}")
        
        # Initialize parent class
        super().__init__(**kwargs)
        
        # Override features in data loader
        self.data_loader.features = selected_features
        
        # Recalculate observation dimension based on selected features
        self._recalculate_observation_dim()
    
    def _recalculate_observation_dim(self):
        """Recalculate observation dimension based on selected features"""
        self.n_features = len(self.selected_features)
        
        # Updated observation size calculation
        obs_size = (
            (self.window_size * self.n_stocks * self.n_features) +  # Historical features
            self.n_stocks * 2 +                                     # Current open prices + gaps
            self.action_dim +                                       # Portfolio weights
            self.n_stocks +                                         # Short position flags
            8                                                       # Market state (8 elements)
        )
        
        self.obs_dim = obs_size
        
        logger.info(f"Recalculated observation dimension: {self.obs_dim}")
        logger.info(f"Features per stock: {self.n_features}")
        logger.info(f"Historical window features: {self.window_size * self.n_stocks * self.n_features}")
    
    def engineer_features(self, df: pd.DataFrame, stock: str) -> pd.DataFrame:
        """
        Override feature engineering to only generate selected features.
        This avoids generating hundreds of unnecessary features.
        """
        # Start with basic OHLCV columns
        df_engineered = df.copy()
        
        # Only generate features that are in our selected_features list
        # This prevents generating hundreds of unnecessary features
        
        # Basic returns (if requested)
        if 'returns_1d' in self.selected_features:
            df_engineered['returns_1d'] = df['close'].pct_change()
        if 'returns_3d' in self.selected_features:
            df_engineered['returns_3d'] = df['close'].pct_change(periods=3)
        if 'returns_5d' in self.selected_features:
            df_engineered['returns_5d'] = df['close'].pct_change(periods=5)
        if 'returns_10d' in self.selected_features:
            df_engineered['returns_10d'] = df['close'].pct_change(periods=10)
        if 'returns_20d' in self.selected_features:
            df_engineered['returns_20d'] = df['close'].pct_change(periods=20)
        
        # Log returns (if requested)
        if 'log_returns_1d' in self.selected_features:
            df_engineered['log_returns_1d'] = np.log(df['close'] / df['close'].shift(1))
        if 'log_returns_5d' in self.selected_features:
            df_engineered['log_returns_5d'] = np.log(df['close'] / df['close'].shift(5))
        
        # Price action features (if requested)
        if 'overnight_gap' in self.selected_features and all(col in df.columns for col in ['open', 'close']):
            df_engineered['overnight_gap'] = df['open'] / df['close'].shift(1) - 1.0
        if 'daily_range' in self.selected_features and all(col in df.columns for col in ['high', 'low', 'open']):
            df_engineered['daily_range'] = (df['high'] - df['low']) / df['open']
        if 'close_position' in self.selected_features and all(col in df.columns for col in ['high', 'low', 'close']):
            df_engineered['close_position'] = (df['close'] - df['low']) / (df['high'] - df['low'] + 1e-8)
        
        # Now filter to only include features that are in our selected_features list
        # and that actually exist in the dataframe
        available_features = [f for f in self.selected_features if f in df_engineered.columns]
        
        # Create filtered dataframe with only selected features
        filtered_df = df_engineered[available_features].copy()
        
        # Ensure 'close' is always first
        if 'close' in filtered_df.columns:
            cols = ['close'] + [c for c in filtered_df.columns if c != 'close']
            filtered_df = filtered_df[cols]
        
        logger.info(f"Generated {len(filtered_df.columns)} features for {stock}: {list(filtered_df.columns)}")
        
        return filtered_df.fillna(0.0)


class FeatureCombinationPPOTrainer(PPOTrainer):
    """PPO Trainer with feature combination support, hyperparameter tuning, and robust early stopping"""
    
    def __init__(self, config: Dict[str, Any], selected_features: List[str]):
        """
        Initialize trainer with selected features and enhanced hyperparameter tuning.
        
        Args:
            config: Training configuration
            selected_features: List of features to use
        """
        self.selected_features = selected_features
        
        # Update config to use custom environment
        config['use_custom_env'] = True
        config['selected_features'] = selected_features
        
        logger.info(f"Initializing Enhanced PPO Trainer with {len(selected_features)} features")
        
        # Initialize parent class but override environment creation
        self.config = config
        self.nan_count = 0
        self.max_nan_resets = 5
        
        # Enhanced early stopping configuration with robust metrics
        self.early_stopping_patience = config.get('early_stopping_patience', 150)
        self.early_stopping_min_delta = config.get('early_stopping_min_delta', 0.005)
        self.best_performance = -float('inf')
        self.patience_counter = 0
        self.performance_history = []
        self.should_stop_early = False
        
        # Robust performance tracking
        self.sharpe_history = []
        self.max_drawdown_history = []
        self.volatility_history = []
        self.best_sharpe = -float('inf')
        self.best_max_drawdown = float('inf')
        
        # KL divergence tracking for policy update control
        self.kl_divergence_history = []
        self.max_kl_divergence = config.get('max_kl_divergence', 0.03)
        self.kl_penalty_coeff = config.get('kl_penalty_coeff', 0.1)
        
        # Reward normalization
        self.reward_mean = 0.0
        self.reward_std = 1.0
        self.reward_clip_range = config.get('reward_clip_range', [-10.0, 10.0])
        self.reward_normalization_window = config.get('reward_normalization_window', 1000)
        self.recent_rewards = []
        
        # Action std annealing
        self.initial_action_std = config.get('action_std', 0.3)
        self.final_action_std = config.get('final_action_std', 0.15)
        self.action_std_decay_steps = config.get('action_std_decay_steps', 500)
        
        # Entropy annealing
        self.initial_entropy_coeff = config.get('entropy_coeff', 0.01)
        self.final_entropy_coeff = config.get('final_entropy_coeff', 0.005)
        self.entropy_decay_steps = config.get('entropy_decay_steps', 800)

        logger.info("Initializing PPO Trainer...")

        # Initialize custom environment with error handling
        try:
            self._initialize_environment()
        except Exception as e:
            logger.error(f"Failed to initialize custom environment: {e}")
            raise

        # Vectorized environment functions
        self.vmap_reset = jax.vmap(self.env.reset, in_axes=(0,))
        self.vmap_step = jax.vmap(self.env.step, in_axes=(0, 0))
        
        # Initialize network
        self.network = ActorCriticLSTM(
            action_dim=self.env.action_dim,
            hidden_size=config.get('hidden_size', 256),
            n_lstm_layers=config.get('n_lstm_layers', 1)
        )

        # Initialize parameters with robust error handling
        self._initialize_parameters()

        # Enhanced optimizer with improved learning rate scheduling
        self.total_steps = config.get('num_updates', 1000)
        
        # Multi-stage learning rate schedule for better learning dynamics
        warmup_steps = config.get('warmup_steps', 50)
        decay_steps = self.total_steps - warmup_steps
        
        # Warmup + cosine decay schedule
        def create_lr_schedule():
            def lr_fn(step):
                # Warmup phase
                if step < warmup_steps:
                    warmup_lr = config.get('learning_rate', 3e-4) * (step / warmup_steps)
                    return warmup_lr
                else:
                    # Cosine decay after warmup
                    decay_step = step - warmup_steps
                    cosine_lr = optax.cosine_decay_schedule(
                        init_value=config.get('learning_rate', 3e-4),
                        decay_steps=decay_steps,
                        alpha=0.05  # Minimum learning rate is 5% of initial
                    )(decay_step)
                    return cosine_lr
            return lr_fn
        
        self.learning_rate_schedule = create_lr_schedule()
        
        # Enhanced optimizer with better stability
        self.optimizer = optax.chain(
            optax.clip_by_global_norm(config.get('max_grad_norm', 0.5)),  # Moderate clipping
            optax.adamw(  # AdamW for better weight decay
                learning_rate=self.learning_rate_schedule,
                eps=1e-8,
                b1=0.9,
                b2=0.999,
                weight_decay=config.get('weight_decay', 1e-5)  # Small weight decay for regularization
            )
        )

        # Create training state
        self.train_state = train_state.TrainState.create(
            apply_fn=self.network.apply,
            params=self.params,
            tx=self.optimizer
        )

        # Initialize RNG
        self.rng = jax.random.PRNGKey(self.config.get('seed', 42))
        
        # Initialize environment and LSTM state
        self._initialize_environment_state()

        # Initialize wandb if enabled
        if config.get('use_wandb', False) and wandb is not None:
            try:
                wandb.init(
                    project=config.get('wandb_project', 'finagent-ppo'),
                    config=config,
                    name=f"ppo-{config.get('feature_combination', 'unknown')}-{int(time.time())}"
                )
                logger.info("Wandb initialized successfully")
            except Exception as e:
                logger.warning(f"Failed to initialize wandb: {e}")
        elif config.get('use_wandb', False) and wandb is None:
            logger.warning("Wandb requested but not available. Install wandb to enable logging.")

        logger.info("Enhanced PPO Trainer initialization complete!")
    
    def normalize_rewards(self, rewards: jnp.ndarray) -> jnp.ndarray:
        """
        Normalize rewards to prevent outlier dominance and improve learning stability.
        
        Args:
            rewards: Raw rewards from environment
            
        Returns:
            Normalized rewards
        """
        # Convert to numpy for easier manipulation
        rewards_np = np.array(rewards)
        
        # Add to recent rewards for running statistics
        self.recent_rewards.extend(rewards_np.flatten())
        
        # Keep only recent window
        if len(self.recent_rewards) > self.reward_normalization_window:
            self.recent_rewards = self.recent_rewards[-self.reward_normalization_window:]
        
        # Update running statistics
        if len(self.recent_rewards) > 10:  # Need some data for statistics
            self.reward_mean = np.mean(self.recent_rewards)
            self.reward_std = np.std(self.recent_rewards) + 1e-8  # Avoid division by zero
        
        # Normalize rewards
        if self.reward_std > 1e-8:
            normalized_rewards = (rewards_np - self.reward_mean) / self.reward_std
        else:
            normalized_rewards = rewards_np
        
        # Clip extreme rewards to prevent outlier dominance
        normalized_rewards = np.clip(
            normalized_rewards, 
            self.reward_clip_range[0], 
            self.reward_clip_range[1]
        )
        
        return jnp.array(normalized_rewards)
    
    def get_current_action_std(self, step: int) -> float:
        """Get current action standard deviation with annealing"""
        if step >= self.action_std_decay_steps:
            return self.final_action_std
        
        # Linear annealing from initial to final
        progress = step / self.action_std_decay_steps
        current_std = self.initial_action_std - (self.initial_action_std - self.final_action_std) * progress
        return current_std
    
    def get_current_entropy_coeff(self, step: int) -> float:
        """Get current entropy coefficient with annealing"""
        if step >= self.entropy_decay_steps:
            return self.final_entropy_coeff
        
        # Linear annealing from initial to final
        progress = step / self.entropy_decay_steps
        current_coeff = self.initial_entropy_coeff - (self.initial_entropy_coeff - self.final_entropy_coeff) * progress
        return current_coeff
    
    def compute_robust_metrics(self, trajectory: Trajectory) -> Dict[str, float]:
        """
        Compute robust performance metrics for early stopping.
        
        Args:
            trajectory: Current trajectory
            
        Returns:
            Dictionary of robust metrics
        """
        # Compute episode returns
        episode_returns = trajectory.rewards.sum(axis=0)  # Sum over time steps
        
        # Sharpe ratio (risk-adjusted return)
        if len(episode_returns) > 1:
            sharpe_ratio = float(jnp.mean(episode_returns) / (jnp.std(episode_returns) + 1e-8))
        else:
            sharpe_ratio = 0.0
        
        # Max drawdown approximation (simplified)
        cumulative_returns = jnp.cumsum(trajectory.rewards, axis=0)
        running_max = jnp.maximum.accumulate(cumulative_returns, axis=0)
        drawdowns = running_max - cumulative_returns
        max_drawdown = float(jnp.max(drawdowns))
        
        # Volatility (standard deviation of returns)
        volatility = float(jnp.std(trajectory.rewards))
        
        # Risk-adjusted return (return / volatility)
        mean_return = float(jnp.mean(trajectory.rewards))
        risk_adjusted_return = mean_return / (volatility + 1e-8)
        
        return {
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'volatility': volatility,
            'risk_adjusted_return': risk_adjusted_return,
            'mean_return': mean_return
        }
    
    def _initialize_parameters(self):
        """Initialize network parameters with comprehensive error handling"""
        logger.info("Initializing network parameters...")
        try:
            # Create dummy input for parameter initialization
            dummy_obs = jnp.zeros((1, self.env.obs_dim))
            dummy_lstm_carry = [
                LSTMState(
                    h=jnp.zeros((1, self.config.get('hidden_size', 256))),
                    c=jnp.zeros((1, self.config.get('hidden_size', 256)))
                ) for _ in range(self.config.get('n_lstm_layers', 1))
            ]
            
            # Initialize parameters with Flax tracing workaround
            rng = jax.random.PRNGKey(42)
            
            # Workaround for Flax tracing error - use jax.jit to avoid tracing issues
            try:
                # Try direct initialization first
                self.params = self.network.init(rng, dummy_obs, dummy_lstm_carry)
                logger.info("Network parameters initialized successfully (direct method)")
            except Exception as trace_error:
                logger.warning(f"Direct initialization failed: {trace_error}")
                logger.info("Attempting alternative initialization method...")
                
                # Alternative method: try using the original initialization approach
                try:
                    # Try using the same approach as in train_ppo_lstm.py
                    dummy_carry = self._create_dummy_carry(1)  # Single environment
                    self.params = self.network.init(rng, dummy_obs, dummy_carry)
                    logger.info("Network parameters initialized successfully (original method)")
                except Exception as orig_error:
                    logger.warning(f"Original initialization failed: {orig_error}")
                    
                    # Skip JIT initialization and go directly to manual initialization
                    logger.info("Attempting manual parameter initialization...")
                    
                    # Manual initialization as fallback
                    self.params = self._manual_init_parameters(rng, dummy_obs, dummy_lstm_carry)
                    logger.info("Network parameters initialized successfully (manual method)")
            
        except Exception as e:
            logger.error(f"Failed to initialize network parameters: {e}")
            raise
    
    def _manual_init_parameters(self, rng_key, dummy_obs, dummy_lstm_carry):
        """Manual parameter initialization as fallback"""
        logger.info("Using manual parameter initialization...")
        
        # Create parameter structure matching ActorCriticLSTM
        hidden_size = self.config.get('hidden_size', 256)
        action_dim = self.env.action_dim
        n_lstm_layers = self.config.get('n_lstm_layers', 1)
        
        # Split RNG for different components
        rng_key, input_rng, lstm_rng, actor_rng, critic_rng = jax.random.split(rng_key, 5)
        
        # Initialize input preprocessing layers
        input_norm_params = {
            'scale': jnp.ones(dummy_obs.shape[-1]),
            'bias': jnp.zeros(dummy_obs.shape[-1])
        }
        
        input_dense_params = {
            'kernel': jax.random.normal(input_rng, (dummy_obs.shape[-1], hidden_size)) * 0.1,
            'bias': jnp.zeros(hidden_size)
        }
        
        # Initialize LSTM layers
        lstm_cells_params = {}
        lstm_layer_norms_params = {}
        
        for i in range(n_lstm_layers):
            lstm_rng, layer_rng = jax.random.split(lstm_rng)
            
            # LSTM cell parameters (input + hidden -> 4 * hidden_size)
            lstm_cells_params[f'layer_{i}'] = {
                'kernel': jax.random.normal(layer_rng, (hidden_size, 4 * hidden_size)) * 0.1,
                'recurrent_kernel': jax.random.normal(layer_rng, (hidden_size, 4 * hidden_size)) * 0.1,
                'bias': jnp.zeros(4 * hidden_size)
            }
            
            # LayerNorm for LSTM outputs
            lstm_layer_norms_params[f'layer_{i}'] = {
                'scale': jnp.ones(hidden_size),
                'bias': jnp.zeros(hidden_size)
            }
        
        # Initialize actor head
        actor_rng1, actor_rng2, actor_rng3 = jax.random.split(actor_rng, 3)
        
        actor_params = {
            'dense1': {
                'kernel': jax.random.normal(actor_rng1, (hidden_size, hidden_size // 2)) * 0.1,
                'bias': jnp.zeros(hidden_size // 2)
            },
            'dense2': {
                'kernel': jax.random.normal(actor_rng2, (hidden_size // 2, hidden_size // 4)) * 0.1,
                'bias': jnp.zeros(hidden_size // 4)
            },
            'output': {
                'kernel': jax.random.normal(actor_rng3, (hidden_size // 4, action_dim)) * 0.1,
                'bias': jnp.zeros(action_dim)
            }
        }
        
        # Initialize critic head
        critic_rng1, critic_rng2, critic_rng3 = jax.random.split(critic_rng, 3)
        
        critic_params = {
            'dense1': {
                'kernel': jax.random.normal(critic_rng1, (hidden_size, hidden_size // 2)) * 0.1,
                'bias': jnp.zeros(hidden_size // 2)
            },
            'dense2': {
                'kernel': jax.random.normal(critic_rng2, (hidden_size // 2, hidden_size // 4)) * 0.1,
                'bias': jnp.zeros(hidden_size // 4)
            },
            'output': {
                'kernel': jax.random.normal(critic_rng3, (hidden_size // 4, 1)) * 0.1,
                'bias': jnp.zeros(1)
            }
        }
        
        return {
            'input_norm': input_norm_params,
            'input_dense': input_dense_params,
            'lstm_cells': lstm_cells_params,
            'lstm_layer_norms': lstm_layer_norms_params,
            'actor_dense1': actor_params['dense1'],
            'actor_dense2': actor_params['dense2'],
            'actor_output': actor_params['output'],
            'critic_dense1': critic_params['dense1'],
            'critic_dense2': critic_params['dense2'],
            'critic_output': critic_params['output']
        }
    
    def _create_dummy_carry(self, batch_size: int) -> List[LSTMState]:
        """Create dummy LSTM carry states"""
        return [
            LSTMState(
                h=jnp.zeros((batch_size, self.config.get('hidden_size', 256))),
                c=jnp.zeros((batch_size, self.config.get('hidden_size', 256)))
            ) for _ in range(self.config.get('n_lstm_layers', 1))
        ]
    
    def _initialize_environment_state(self):
        """Initialize environment and LSTM state"""
        logger.info("Initializing environment state...")
        try:
            # Initialize environment states
            rng = jax.random.split(jax.random.PRNGKey(42), self.config.get('n_envs', 1))
            self.env_states, self.obs = self.vmap_reset(rng)
            
            # Clean environment observations (handle inf/nan)
            self.obs = jnp.where(jnp.isnan(self.obs), 0.0, self.obs)
            self.obs = jnp.where(jnp.isinf(self.obs), 0.0, self.obs)
            
            # Initialize LSTM carry states
            self.collector_carry = self._create_dummy_carry(self.config.get('n_envs', 1))
            
            logger.info("Environment states initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize environment states: {e}")
            raise
    
    def _get_env_config(self) -> Dict[str, Any]:
        """Get environment configuration with feature selection"""
        return {
            'data_root': self.config['data_root'],
            'stocks': self.config.get('stocks', None),
            'features': self.config.get('features', None),
            'initial_cash': self.config.get('initial_cash', 1000000.0),
            'window_size': self.config.get('window_size', 30),
            'start_date': self.config['train_start_date'],
            'end_date': self.config['train_end_date'],
            'transaction_cost_rate': self.config.get('transaction_cost_rate', 0.005),
            'sharpe_window': self.config.get('sharpe_window', 252),
            'use_all_features': self.config.get('use_all_features', True)
        }
    
    def _initialize_environment(self):
        """Initialize custom environment with feature selection"""
        try:
            env_config = self._get_env_config()
            
            # Use custom environment class with selected features
            self.env = CustomPortfolioEnv(self.selected_features, **env_config)
            logger.info(f"Custom environment initialized: obs_dim={self.env.obs_dim}, action_dim={self.env.action_dim}")
            logger.info(f"Using {len(self.selected_features)} features: {self.selected_features[:5]}...")
            
        except Exception as e:
            logger.error(f"Failed to initialize custom environment: {e}")
            raise
    
    def check_early_stopping(self, trajectory: Trajectory, update_step: int) -> bool:
        """
        Enhanced early stopping with robust metrics to avoid overfitting to spurious patterns.
        
        Args:
            trajectory: Current trajectory for robust metric computation
            update_step: Current training step
            
        Returns:
            True if training should stop early, False otherwise
        """
        # Compute robust metrics
        metrics = self.compute_robust_metrics(trajectory)
        
        # Track metrics history
        self.sharpe_history.append(metrics['sharpe_ratio'])
        self.max_drawdown_history.append(metrics['max_drawdown'])
        self.volatility_history.append(metrics['volatility'])
        
        # Keep only recent history for trend analysis
        window_size = 30
        if len(self.sharpe_history) > window_size:
            self.sharpe_history = self.sharpe_history[-window_size:]
            self.max_drawdown_history = self.max_drawdown_history[-window_size:]
            self.volatility_history = self.volatility_history[-window_size:]
        
        # Primary stopping criterion: Sharpe ratio improvement
        current_sharpe = metrics['sharpe_ratio']
        sharpe_improvement = current_sharpe - self.best_sharpe
        
        if sharpe_improvement > self.early_stopping_min_delta:
            # Significant improvement in risk-adjusted returns
            self.best_sharpe = current_sharpe
            self.best_performance = current_sharpe  # For compatibility
            self.patience_counter = 0
            logger.info(f"New best Sharpe ratio: {current_sharpe:.4f} (improvement: {sharpe_improvement:.4f})")
        else:
            # No significant improvement
            self.patience_counter += 1
        
        # Secondary criterion: Check for overfitting indicators
        overfitting_detected = False
        
        # Check 1: Excessive volatility increase (potential overfitting to noise)
        if len(self.volatility_history) >= 10:
            recent_vol = np.mean(self.volatility_history[-5:])
            early_vol = np.mean(self.volatility_history[:5])
            if recent_vol > early_vol * 2.0:  # Volatility doubled
                logger.warning(f"Excessive volatility increase detected: {early_vol:.4f} -> {recent_vol:.4f}")
                overfitting_detected = True
        
        # Check 2: Max drawdown getting worse
        if len(self.max_drawdown_history) >= 10:
            recent_dd = np.mean(self.max_drawdown_history[-5:])
            early_dd = np.mean(self.max_drawdown_history[:5])
            if recent_dd > early_dd * 1.5:  # Drawdown increased by 50%
                logger.warning(f"Max drawdown worsening: {early_dd:.4f} -> {recent_dd:.4f}")
                overfitting_detected = True
        
        # Check 3: Sharpe ratio declining trend
        if len(self.sharpe_history) >= 15:
            recent_sharpe = np.mean(self.sharpe_history[-5:])
            mid_sharpe = np.mean(self.sharpe_history[-10:-5])
            early_sharpe = np.mean(self.sharpe_history[-15:-10])
            
            if recent_sharpe < mid_sharpe < early_sharpe:  # Declining trend
                logger.warning(f"Sharpe ratio declining trend: {early_sharpe:.4f} -> {mid_sharpe:.4f} -> {recent_sharpe:.4f}")
                overfitting_detected = True
        
        # Early stopping decision
        if self.patience_counter >= self.early_stopping_patience:
            logger.info(f"Early stopping triggered after {self.patience_counter} steps without Sharpe improvement")
            logger.info(f"Best Sharpe: {self.best_sharpe:.4f}, Current: {current_sharpe:.4f}")
            return True
        
        # Overfitting-based early stopping (more aggressive)
        if overfitting_detected and self.patience_counter >= self.early_stopping_patience // 2:
            logger.info(f"Early stopping triggered due to overfitting indicators")
            logger.info(f"Metrics - Sharpe: {current_sharpe:.4f}, Vol: {metrics['volatility']:.4f}, DD: {metrics['max_drawdown']:.4f}")
            return True
        
        return False
    
    def get_current_learning_rate(self, step: int) -> float:
        """Get current learning rate from schedule"""
        return self.learning_rate_schedule(step)
    
    def train(self):
        """Override train method to include early stopping"""
        logger.info("Starting PPO training with early stopping...")

        # Training configuration
        num_updates = self.config.get('num_updates', 1000)
        log_interval = self.config.get('log_interval', 10)
        save_interval = self.config.get('save_interval', 50)

        # Calculate minibatches
        n_steps = self.config.get('n_steps', 128)
        n_envs = self.config.get('n_envs', 8)
        ppo_batch_size = self.config.get('ppo_batch_size', 256)
        num_minibatches = max(1, (n_steps * n_envs) // ppo_batch_size)

        logger.info(f"Training configuration: {num_updates} updates, {num_minibatches} minibatches")
        logger.info(f"Early stopping patience: {self.early_stopping_patience} steps")

        for update in range(num_updates):
            start_time = time.time()

            try:
                # Split RNG for collection and training
                self.rng, collect_rng = random.split(self.rng)
            
                # Collect trajectory
                trajectory, self.env_states, self.obs, self.collector_carry = self.collect_trajectory(
                    self.train_state, self.env_states, self.obs, self.collector_carry, collect_rng
                )

                # Apply reward normalization to prevent outlier dominance
                trajectory = trajectory._replace(rewards=self.normalize_rewards(trajectory.rewards))

                # Debug NaN sources periodically
                if update % 20 == 0:
                    try:
                        _, last_values, _ = self.train_state.apply_fn(
                            self.train_state.params, self.obs, self.collector_carry
                        )

                        if trajectory.obs.shape[0] > 0:
                            self.debug_nan_sources(
                                trajectory.obs[0], trajectory.actions[0],
                                trajectory.rewards[0], trajectory.values[0], trajectory.log_probs[0]
                            )
                    except Exception as e:
                        # Silent error handling to avoid JAX tracing issues
                        pass

                # Get bootstrap values for GAE
                try:
                    _, last_values, _ = self.train_state.apply_fn(
                        self.train_state.params, self.obs, self.collector_carry
                    )
                    last_values = jnp.where(jnp.isnan(last_values), 0.0, last_values)
                except Exception as e:
                    # Silent fallback to avoid JAX tracing issues
                    last_values = jnp.zeros(self.config.get('n_envs', 8))

                # Check for NaN parameters before training
                self.train_state, self.rng = self.check_and_reset_nan_params(self.train_state, self.rng)
            
                # Get current hyperparameters for this step
                current_action_std = self.get_current_action_std(update)
                current_entropy_coeff = self.get_current_entropy_coeff(update)
                
                # Update config with current hyperparameters
                self.config['action_std'] = current_action_std
                self.config['entropy_coeff'] = current_entropy_coeff
            
                # Perform PPO training step with enhanced hyperparameters
                self.rng, train_rng = random.split(self.rng)
                self.train_state, metrics = self.train_step(
                    self.train_state, trajectory, last_values, train_rng, num_minibatches
                )
                
                # Track KL divergence for policy update control
                if 'approx_kl' in metrics:
                    self.kl_divergence_history.append(float(metrics['approx_kl']))
                    if len(self.kl_divergence_history) > 50:
                        self.kl_divergence_history = self.kl_divergence_history[-50:]
            
                # Check for NaN parameters after training
                self.train_state, self.rng = self.check_and_reset_nan_params(self.train_state, self.rng)
                
                # Memory cleanup for A100
                if self.config.get('memory_efficient', False):
                    jax.clear_caches()  # Clear JAX caches
                    import gc
                    gc.collect()  # Force garbage collection
            
                # Logging and early stopping check
                if update % log_interval == 0:
                    elapsed = time.time() - start_time

                    # Compute trajectory statistics
                    avg_reward = float(trajectory.rewards.mean())
                    max_return = float(trajectory.rewards.sum(axis=0).max())
                    total_loss = float(metrics.get('total_loss', 0.0))
                    policy_loss = float(metrics.get('policy_loss', 0.0))
                    value_loss = float(metrics.get('value_loss', 0.0))
                    
                    # Get current hyperparameters
                    current_lr = self.get_current_learning_rate(update)
                    current_action_std = self.get_current_action_std(update)
                    current_entropy_coeff = self.get_current_entropy_coeff(update)
                    
                    # Compute robust metrics
                    robust_metrics = self.compute_robust_metrics(trajectory)
                    
                    # Get KL divergence info
                    avg_kl = np.mean(self.kl_divergence_history) if self.kl_divergence_history else 0.0
                    max_kl = np.max(self.kl_divergence_history) if self.kl_divergence_history else 0.0

                    logger.info(
                        f"Update {update}/{num_updates} | "
                        f"Time: {elapsed:.2f}s | "
                        f"LR: {current_lr:.6f} | "
                        f"Action Std: {current_action_std:.3f} | "
                        f"Entropy Coeff: {current_entropy_coeff:.4f} | "
                        f"Total Loss: {total_loss:.4f} | "
                        f"Policy Loss: {policy_loss:.4f} | "
                        f"Value Loss: {value_loss:.4f} | "
                        f"Avg Reward: {avg_reward:.4f} | "
                        f"Sharpe: {robust_metrics['sharpe_ratio']:.4f} | "
                        f"Max DD: {robust_metrics['max_drawdown']:.4f} | "
                        f"Vol: {robust_metrics['volatility']:.4f} | "
                        f"KL: {avg_kl:.4f} (max: {max_kl:.4f}) | "
                        f"Patience: {self.patience_counter}/{self.early_stopping_patience}"
                    )

                    # Check early stopping using robust metrics
                    if self.check_early_stopping(trajectory, update):
                        logger.info(f"Early stopping triggered at update {update}")
                        logger.info(f"Training completed early after {update} updates")
                        break

                    # Log to wandb if enabled
                    if self.config.get('use_wandb', False) and wandb is not None:
                        try:
                            wandb_log = {
                                # Hyperparameters
                                "hyperparams/learning_rate": current_lr,
                                "hyperparams/action_std": current_action_std,
                                "hyperparams/entropy_coeff": current_entropy_coeff,
                                
                                # Losses
                                "losses/total_loss": total_loss,
                                "losses/policy_loss": policy_loss,
                                "losses/value_loss": value_loss,
                                "losses/entropy_loss": float(metrics.get('entropy_loss', 0.0)),
                                "losses/approx_kl": float(metrics.get('approx_kl', 0.0)),
                                "losses/clip_fraction": float(metrics.get('clip_fraction', 0.0)),
                                "losses/avg_kl_divergence": avg_kl,
                                "losses/max_kl_divergence": max_kl,
                                
                                # Basic metrics
                                "rollout/avg_reward": avg_reward,
                                "rollout/max_reward": float(trajectory.rewards.max()),
                                "rollout/min_reward": float(trajectory.rewards.min()),
                                "rollout/avg_episode_return": float(trajectory.rewards.sum(axis=0).mean()),
                                "rollout/max_episode_return": max_return,
                                "rollout/avg_portfolio_value": float(self.env_states.portfolio_value.mean()),
                                
                                # Robust metrics
                                "robust/sharpe_ratio": robust_metrics['sharpe_ratio'],
                                "robust/max_drawdown": robust_metrics['max_drawdown'],
                                "robust/volatility": robust_metrics['volatility'],
                                "robust/risk_adjusted_return": robust_metrics['risk_adjusted_return'],
                                "robust/mean_return": robust_metrics['mean_return'],
                                
                                # Reward normalization
                                "reward_norm/reward_mean": self.reward_mean,
                                "reward_norm/reward_std": self.reward_std,
                                
                                # Early stopping
                                "early_stopping/patience_counter": self.patience_counter,
                                "early_stopping/best_performance": self.best_performance,
                                "early_stopping/best_sharpe": self.best_sharpe,
                                
                                "global_step": update
                            }
                            wandb.log(wandb_log)
                        except Exception as e:
                            logger.warning(f"Wandb logging failed: {e}")
            
                # Save model periodically
                if update % save_interval == 0 and update > 0:
                    try:
                        self.save_model(f"ppo_model_update_{update}")
                    except Exception as e:
                        # Silent error handling to avoid JAX tracing issues
                        pass

            except Exception as e:
                # Silent error handling to avoid JAX tracing issues
                # Continue training despite errors
                continue

        logger.info("Training complete!")

        # Final cleanup
        if self.config.get('use_wandb', False) and wandb is not None:
            try:
                wandb.finish()
            except Exception as e:
                logger.warning(f"Wandb cleanup failed: {e}")


def run_training_with_combination(feature_combination: str, config: Dict[str, Any]):
    """
    Run training with a specific feature combination.
    
    Args:
        feature_combination: Feature combination string
        config: Training configuration
    """
    logger.info(f"Starting training with feature combination: {feature_combination}")
    
    # Initialize feature selector
    feature_selector = FeatureSelector()
    
    # Get selected features
    try:
        selected_features = feature_selector.get_features_for_combination(feature_combination)
        logger.info(f"Selected {len(selected_features)} features for training")
        
        # Log feature distribution by category
        for category, info in feature_selector.feature_categories.items():
            category_features = [f for f in selected_features if f in info['features']]
            logger.info(f"  {category}: {len(category_features)} features")
            
    except Exception as e:
        logger.error(f"Failed to get features for combination '{feature_combination}': {e}")
        feature_selector.print_available_combinations()
        raise
    
    # Update config with feature combination info
    config['feature_combination'] = feature_combination
    config['selected_features'] = selected_features
    config['model_name'] = f"ppo_lstm_{feature_combination.replace('+', '_')}"
    
    # Initialize trainer
    try:
        trainer = FeatureCombinationPPOTrainer(config, selected_features)
        logger.info("Feature combination PPO trainer initialized successfully")
        
    except Exception as e:
        logger.error(f"Failed to initialize trainer: {e}")
        raise
    
    # Run training
    try:
        logger.info("Starting training...")
        trainer.train()
        logger.info("Training completed successfully!")
        
        # Save final model
        trainer.save_model(f"final_model_{feature_combination.replace('+', '_')}")
        
    except Exception as e:
        logger.error(f"Training failed: {e}")
        raise


def main():
    """Main function with command line argument parsing"""
    parser = argparse.ArgumentParser(
        description="Train PPO LSTM with different feature combinations",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python train_ppo_feature_combinations.py --feature_combination ohlcv+technical
  python train_ppo_feature_combinations.py --feature_combination all --num_updates 500
  python train_ppo_feature_combinations.py --feature_combination ohlcv+financial+sentiment --use_wandb
        """
    )
    
    # Feature combination argument
    parser.add_argument(
        '--feature_combination', 
        type=str, 
        default='ohlcv+technical',
        help='Feature combination to use (e.g., ohlcv+technical, all, etc.)'
    )
    
    # Training configuration arguments
    parser.add_argument('--num_updates', type=int, default=1000, help='Number of training updates')
    parser.add_argument('--learning_rate', type=float, default=3e-4, help='Learning rate')
    parser.add_argument('--n_envs', type=int, default=8, help='Number of parallel environments')
    parser.add_argument('--n_steps', type=int, default=32, help='Number of steps per environment')
    parser.add_argument('--ppo_epochs', type=int, default=4, help='PPO epochs per update')
    parser.add_argument('--ppo_batch_size', type=int, default=128, help='PPO batch size')
    parser.add_argument('--hidden_size', type=int, default=256, help='LSTM hidden size')
    parser.add_argument('--n_lstm_layers', type=int, default=2, help='Number of LSTM layers')
    
    # Data configuration arguments
    parser.add_argument('--data_root', type=str, default='processed_data/', help='Data root directory')
    parser.add_argument('--train_start_date', type=str, default='2024-06-06', help='Training start date')
    parser.add_argument('--train_end_date', type=str, default='2025-03-06', help='Training end date')
    parser.add_argument('--window_size', type=int, default=30, help='Observation window size')
    
    # Early stopping arguments
    parser.add_argument('--early_stopping_patience', type=int, default=150, help='Early stopping patience (steps without improvement)')
    parser.add_argument('--early_stopping_min_delta', type=float, default=0.005, help='Minimum improvement threshold for early stopping')
    
    # Hyperparameter tuning arguments
    parser.add_argument('--gae_lambda', type=float, default=0.97, help='GAE lambda for advantage estimation')
    parser.add_argument('--clip_eps', type=float, default=0.15, help='PPO clipping epsilon')
    parser.add_argument('--max_grad_norm', type=float, default=0.5, help='Maximum gradient norm for clipping')
    parser.add_argument('--warmup_steps', type=int, default=50, help='Learning rate warmup steps')
    parser.add_argument('--weight_decay', type=float, default=1e-5, help='Weight decay for regularization')
    parser.add_argument('--max_kl_divergence', type=float, default=0.03, help='Maximum KL divergence threshold')
    parser.add_argument('--kl_penalty_coeff', type=float, default=0.1, help='KL penalty coefficient')
    
    # Reward normalization arguments
    parser.add_argument('--reward_clip_min', type=float, default=-10.0, help='Minimum reward clipping value')
    parser.add_argument('--reward_clip_max', type=float, default=10.0, help='Maximum reward clipping value')
    parser.add_argument('--reward_normalization_window', type=int, default=1000, help='Window size for reward normalization')
    
    # Annealing arguments
    parser.add_argument('--final_action_std', type=float, default=0.15, help='Final action standard deviation')
    parser.add_argument('--action_std_decay_steps', type=int, default=500, help='Steps to anneal action std')
    parser.add_argument('--final_entropy_coeff', type=float, default=0.005, help='Final entropy coefficient')
    parser.add_argument('--entropy_decay_steps', type=int, default=800, help='Steps to anneal entropy coefficient')
    
    # Other arguments
    parser.add_argument('--use_wandb', action='store_true', help='Use Weights & Biases logging')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    parser.add_argument('--model_dir', type=str, default='models', help='Model save directory')
    parser.add_argument('--list_combinations', action='store_true', help='List available feature combinations')
    
    args = parser.parse_args()
    
    # List available combinations if requested
    if args.list_combinations:
        feature_selector = FeatureSelector()
        feature_selector.print_available_combinations()
        return
    
    # Create training configuration
    config = {
        # Environment settings
        'seed': args.seed,
        'data_root': args.data_root,
        'stocks': None,  # Will be loaded from stocks.txt
        'train_start_date': args.train_start_date,
        'train_end_date': args.train_end_date,
        'window_size': args.window_size,
        'transaction_cost_rate': 0.005,
        'sharpe_window': 252,
        
        # Data loading settings
        'use_all_features': False,  # We'll use custom feature selection
        'save_cache': True,
        'cache_format': 'hdf5',
        'force_reload': False,
        'preload_to_gpu': True,
        
        # Training environment
        'n_envs': args.n_envs,
        'n_steps': args.n_steps,
        
        # PPO hyperparameters - Enhanced for stable learning and hyperparameter tuning
        'num_updates': args.num_updates,
        'gamma': 0.99,
        'gae_lambda': args.gae_lambda,  # Increased for smoother advantage estimation
        'clip_eps': args.clip_eps,  # Moderate clipping for controlled policy updates
        'ppo_epochs': args.ppo_epochs,
        'ppo_batch_size': args.ppo_batch_size,
        'learning_rate': args.learning_rate,
        'max_grad_norm': args.max_grad_norm,  # Moderate gradient clipping
        'value_coeff': 0.5,
        'entropy_coeff': 0.01,  # Will be annealed during training
        'action_std': 0.3,  # Will be annealed during training
        
        # Enhanced hyperparameter tuning
        'warmup_steps': args.warmup_steps,  # Learning rate warmup
        'weight_decay': args.weight_decay,  # Weight decay for regularization
        'max_kl_divergence': args.max_kl_divergence,  # KL divergence threshold
        'kl_penalty_coeff': args.kl_penalty_coeff,  # KL penalty coefficient
        
        # Reward normalization
        'reward_clip_range': [args.reward_clip_min, args.reward_clip_max],  # Clip extreme rewards
        'reward_normalization_window': args.reward_normalization_window,  # Window for reward statistics
        
        # Annealing schedules
        'final_action_std': args.final_action_std,  # Final action standard deviation
        'action_std_decay_steps': args.action_std_decay_steps,  # Steps to anneal action std
        'final_entropy_coeff': args.final_entropy_coeff,  # Final entropy coefficient
        'entropy_decay_steps': args.entropy_decay_steps,  # Steps to anneal entropy
        
        # Early stopping configuration
        'early_stopping_patience': args.early_stopping_patience,  # Very liberal patience
        'early_stopping_min_delta': args.early_stopping_min_delta,  # Very small improvement threshold
        
        # Network architecture
        'hidden_size': args.hidden_size,
        'n_lstm_layers': args.n_lstm_layers,
        
        # GPU optimizations - Disabled for Metal compatibility
        'use_mixed_precision': False,
        'compile_mode': 'default',
        'memory_efficient': False,
        'gradient_checkpointing': False,
        
        # Logging and monitoring
        'use_wandb': args.use_wandb,
        'log_interval': 20,
        'save_interval': 100,
        'model_dir': args.model_dir,
    }
    
    # Run training
    try:
        run_training_with_combination(args.feature_combination, config)
        
    except KeyboardInterrupt:
        logger.info("Training interrupted by user")
        
    except Exception as e:
        logger.error(f"Training failed with error: {e}")
        raise


if __name__ == "__main__":
    main()
